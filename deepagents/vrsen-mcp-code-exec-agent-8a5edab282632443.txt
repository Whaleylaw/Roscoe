Directory structure:
└── vrsen-mcp-code-exec-agent/
    ├── README.md
    ├── agency.py
    ├── AGENTS.md
    ├── CLAUDE.md
    ├── Dockerfile
    ├── main.py
    ├── pyproject.toml
    ├── requirements.txt
    ├── shared_instructions.md
    ├── .dockerignore
    ├── .env.template
    ├── mnt/
    │   └── skills/
    │       └── .gitkeep
    ├── sales_ops/
    │   ├── __init__.py
    │   ├── instructions.md
    │   └── sales_ops.py
    ├── sales_ops_direct_mcp/
    │   ├── __init__.py
    │   ├── instructions.md
    │   └── sales_ops_direct_mcp.py
    ├── servers/
    │   ├── gdrive/
    │   │   ├── __init__.py
    │   │   ├── gdrive_read_file.py
    │   │   ├── gdrive_read_sheet.py
    │   │   ├── gdrive_search.py
    │   │   ├── gdrive_update_cell.py
    │   │   └── server.py
    │   └── notion/
    │       ├── __init__.py
    │       ├── notion_create_comment.py
    │       ├── notion_create_database.py
    │       ├── notion_create_pages.py
    │       ├── notion_duplicate_page.py
    │       ├── notion_fetch.py
    │       ├── notion_get_comments.py
    │       ├── notion_get_self.py
    │       ├── notion_get_teams.py
    │       ├── notion_get_user.py
    │       ├── notion_get_users.py
    │       ├── notion_list_agents.py
    │       ├── notion_move_pages.py
    │       ├── notion_search.py
    │       ├── notion_update_database.py
    │       ├── notion_update_page.py
    │       └── server.py
    ├── .claude/
    │   ├── README.md
    │   └── agents/
    │       ├── agent-creator.md
    │       ├── api-researcher.md
    │       ├── instructions-writer.md
    │       ├── prd-creator.md
    │       ├── qa-tester.md
    │       └── tools-creator.md
    └── .cursor/
        ├── commands/
        │   ├── add-mcp.md
        │   ├── create-prd.md
        │   ├── mcp-code-exec.md
        │   ├── productize.md
        │   └── write-instructions.md
        └── rules/
            └── workflow.mdc

================================================
FILE: README.md
================================================
# Sales Ops Agent - MCP Code Execution Pattern

**98% reduction in token consumption while giving agents more autonomy and flexibility.**

> ⚠️ **IMPORTANT: Keep this repository PRIVATE**
>
> This implementation stores OAuth credentials in `./mnt/mcp-creds/` which are currently committed to the repository. Proper OAuth flow for MCP servers is coming soon to the Agencii platform. Until then, ensure your repository visibility is set to **private** to protect your credentials.

This implementation follows Anthropic's [Code Execution with MCP](https://www.anthropic.com/engineering/code-execution-with-mcp) pattern, where agents write code to interact with MCP servers instead of making direct tool calls. The agent discovers tools by exploring the filesystem and loads only what it needs for each task.

## Why This Approach?

**Traditional MCP (Direct Tool Calls):**

- Loads all 19 tool definitions upfront (~150K tokens)
- Every intermediate result flows through model context
- Example: Copying a transcript consumes 32K tokens

**Code Execution Pattern:**

- Loads tools on-demand from filesystem (~2K tokens)
- Processes data in execution environment
- Same task consumes 4K tokens with skills, 12K without

## Architecture

```
sales_ops agent
├── IPythonInterpreter (code execution)
├── PersistentShellTool (file discovery)
└── MCP Servers (as code APIs)
    ├── servers/notion/ (15 tools)
    │   ├── search.py
    │   ├── fetch.py
    │   └── ... (other tools)
    └── servers/gdrive/ (4 tools)
        ├── search.py
        ├── read_file.py
        ├── read_sheet.py
        └── update_cell.py
```

## Quick Start

### 1. Clone Repository

```bash
git clone <your-repo>
cd code-exec-agent
```

### 2. Setup Environment

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configure Credentials

Add to `.env`:

```bash
OPENAI_API_KEY=your-openai-key

# Google Drive (required)
GDRIVE_CREDENTIALS_JSON={"installed":{"client_id":"...","client_secret":"...","redirect_uris":["http://localhost"]}}

# Notion (uses OAuth via mcp-remote - auto-configured)
```

**Getting Google Drive Credentials:**

1. Create Google Cloud project
2. Enable Google Drive API, Google Sheets API, Google Docs API
3. Create OAuth Client ID for "Desktop App"
4. Download JSON and add to `GDRIVE_CREDENTIALS_JSON`

### 4. Authenticate Google Drive

```bash
npx @isaacphi/mcp-gdrive
# Follow OAuth flow in browser
# Press Ctrl+C after "Setting up automatic token refresh"
```

### 5. Test the Agent

```bash
python agency.py
```

## Example Test Task

**Task:** Add transcript from Google Doc to Notion page

```
Add this transcript from this Google doc https://docs.google.com/document/d/YOUR_DOC_ID
to this Notion page https://www.notion.so/YOUR_PAGE_ID
```

**What Happens:**

1. Agent checks `./mnt/skills/` for existing skill
2. If not found, reads only needed tools:
   - `servers/gdrive/read_file.py`
   - `servers/notion/update_page.py`
3. Writes code in IPythonInterpreter:

```python
from servers.gdrive import read_file
from servers.notion import update_page

# Read transcript (stays in execution environment)
transcript = await read_file(fileId="YOUR_DOC_ID")

# Update Notion page
await update_page(data={
    "page_id": "YOUR_PAGE_ID",
    "command": "replace_content",
    "new_str": transcript
})
```

4. Suggests saving as reusable skill
5. Next time: uses skill directly (4K tokens vs 12K)

## Convert a Traditional MCP Agent to this Pattern with Cursor

Follow this step-by-step workflow using Cursor's AI commands:

### Step 1: Create Agent MVP (If not already created)

```
Create a sales_ops agent with 2 built-in tools: IPythonInterpreter and PersistentShellTool
```

**Why these tools:**

- `IPythonInterpreter` - Executes code with top-level await
- `PersistentShellTool` - Discovers files and reads tool definitions

### Step 2: Add MCP Servers Using Code Execution Pattern

```
/mcp-code-exec

Add the following mcp servers to sales_ops agent:
https://developers.notion.com/docs/get-started-with-mcp
https://github.com/isaacphi/mcp-gdrive
```

**What this does:**

- Creates `servers/notion/` with 15 tool files
- Creates `servers/gdrive/` with 4 tool files
- Each tool is a Python file with async function
- Auto-creates `server.py` for connection management
- Tests server connections

### Step 3: Write Agent Instructions

```
/write-instructions @sales_ops.py

Main role: Performing operational tasks for the team
Business goal: Improve efficiency
Process:
1. Discover skills in ./mnt/skills folder
2. Use skill if it matches task
3. If no skills found, read ONLY necessary tool files
4. Import and combine tools in IPythonInterpreter
5. Suggest new skills to be added

Keep these instructions short. Don't add mcp usage examples or don't list all mcps. Agent should discover them autonomously.

Agent should also minimize token consumption by performing as few tool calls as possible and only reading the necessary tool files to complete the task.

Output: Summary + skill suggestions
```

**Key workflow points:**

- **Skills-first approach** - Always check `./mnt/skills/` first
- **Progressive disclosure** - Only read tools you need
- **Self-improvement** - Create reusable skills over time
- **Minimize token consumption** - Agent shouldn't read too many files

### Step 4: Handle Authentication

If you see authentication errors:

```
I added secrets, please retest google drive tools make sure each tool is production ready
```

Then authenticate:

```bash
npx @isaacphi/mcp-gdrive
```

### Step 5: Test and Deploy

```bash
# Test locally
python agency.py

# Deploy to Agency Swarm platform
git push origin main
# Go to platform.agency-swarm.ai
# Create new agency from repo
# Add environment variables
```

## How It Works

### Traditional Direct MCP (Comparison Agent)

```
User: Add transcript to Notion

Agent → MCP: gdrive.read_file(docId)
MCP → Agent: [Full 50KB transcript in context]

Agent → MCP: notion.update_page(pageId, transcript)
        [Agent rewrites full 50KB transcript again]

Result: 32,000 tokens consumed
```

### Code Execution Pattern (This Implementation)

```
User: Add transcript to Notion

Agent → Shell: ls ./mnt/skills/
Agent → Shell: cat servers/gdrive/read_file.py

Agent → IPython:
    from servers.gdrive import read_file
    from servers.notion import update_page
    transcript = await read_file(fileId="...")
    await update_page(data={...})

Result: 12,000 tokens (first time), 4,000 tokens (with skill)
```

### Progressive Disclosure

Instead of loading all 19 tools upfront:

```
# Traditional: All tools loaded immediately
✗ 150K tokens - Full definitions for all 19 tools in context

# Code Execution: Load on demand
✓ 2K tokens - List directory to see available tools
✓ Read only the 2 files needed for current task
```

### Skills System

Agent builds its own library of reusable functions:

```
./mnt/skills/
├── copy_gdrive_to_notion.py
├── export_sheet_to_csv.py
└── search_and_email_results.py
```

Skills persist across chat sessions. Each completed task is an opportunity to create a new skill.

## Performance Comparison

**Test Task:** Copy Google Doc transcript to Notion page

| Approach       | First Run  | With Skill | Reduction |
| -------------- | ---------- | ---------- | --------- |
| Direct MCP     | 32K tokens | 32K tokens | -         |
| Code Execution | 12K tokens | 4K tokens  | **88%**   |

## When to Use This Approach

✅ **Use Code Execution Pattern for:**

- Operations agents (data sync, reporting)
- Research agents (gather, analyze, summarize)
- Analytics agents (query, transform, visualize)
- Agents with 10+ tools
- Tasks with large data processing

❌ **Use Traditional MCP for:**

- Simple customer support (3-5 tools)
- Single-purpose agents
- Tasks requiring immediate consistency
- When infrastructure overhead isn't acceptable

## Agent Workflow

The agent follows this process for every task:

```
1. Check Skills
   └─ ls ./mnt/skills/
   └─ If match found → Execute skill → Done

2. Identify Tools Needed
   └─ Based on task: Notion? Drive? Both?

3. Read ONLY Necessary Tools
   └─ cat servers/notion/fetch.py
   └─ cat servers/gdrive/read_file.py
   └─ DO NOT read server.py or other files

4. Combine Tools in Code
   └─ Write Python code in IPythonInterpreter
   └─ Use await directly (top-level await enabled)

5. Suggest New Skill
   └─ Analyze workflow
   └─ Propose reusable function
   └─ Save to ./mnt/skills/
```

## Available Tools

### Notion MCP (15 tools)

**Content Operations:**

- `search()` - Semantic search across workspace
- `fetch()` - Get page/database details
- `create_pages()` - Create new pages
- `update_page()` - Update properties/content
- `move_pages()` - Move to new parent
- `duplicate_page()` - Duplicate page

**Database Operations:**

- `create_database()` - Create with schema
- `update_database()` - Update schema

**Comments:**

- `create_comment()` - Add comment
- `get_comments()` - Get all comments

**Workspace:**

- `get_teams()` - List teams
- `get_users()` - List users
- `list_agents()` - List custom agents
- `get_self()` - Get bot info
- `get_user()` - Get specific user

### Google Drive MCP (4 tools)

**Drive:**

- `search()` - Search files
- `read_file()` - Read contents

**Sheets:**

- `read_sheet()` - Read spreadsheet
- `update_cell()` - Update cell value

## Project Structure

```
code-exec-agent/
├── sales_ops/                    # Main agent
│   ├── sales_ops.py             # Agent configuration
│   ├── instructions.md          # Agent prompt (key to performance)
│   └── tools/                   # Built-in tools (empty - uses framework)
├── servers/                     # MCP servers as code
│   ├── notion/
│   │   ├── server.py           # Connection management
│   │   ├── __init__.py         # Exports all tools
│   │   ├── search.py           # Individual tool
│   │   └── ... (15 tools)
│   └── gdrive/
│       ├── server.py
│       ├── __init__.py
│       └── ... (4 tools)
├── mnt/
│   ├── skills/                  # Agent-created reusable functions
│   └── mcp-creds/              # OAuth tokens (auto-managed)
├── agency.py                    # Entry point
├── .env                         # Credentials
└── requirements.txt
```

## Troubleshooting

### Agent reads too many files

**Problem:** Agent reads server.py, README.md, etc.

**Solution:** Update instructions.md:

```markdown
**DO NOT** read any other tool, readme, or server files to avoid extra token consumption.
Only read what you need for the specific task.
```

### OAuth/Authentication errors

**Problem:** OAuth/Authentication are not working after deployment.

**Solution:**

1. Ensure all OAuth tokens are saved to `./mnt/mcp-creds/`
2. Ensure persistent storage is enabled under "Agency" tab

or

1. Trigg OAuth flow again locally
2. Commit and deploy to Agencii.ai
3. Test in another chat

### Agent doesn't use skills

**Problem:** Persistent storage is not enabled so skills are not saved.

**Solution:**

1. Open your agency on Agencii.ai
2. Enable storage under "Agency" tab
3. Wait for build to complete
4. Tell your agent to save the skill
5. Test in another chat

## Deployment

### Option 1: Local Development

```bash
python agency.py
```

### Option 2: Agency Swarm Platform

1. Push to GitHub (private repo)
2. Go to https://agencii.ai
3. Create new agency from repo
4. Add environment variables
5. Deploy

**Platform Benefits:**

- Persistent `./mnt/` storage (skills preserved)
- Automatic scaling
- Built-in tracing & analytics
- No infrastructure management

## Performance Tips

1. **Write clear instructions** - Prompting is key for this pattern
2. **Build skills progressively** - Start simple, improve over time
3. **Use specific task descriptions** - Help agent identify needed tools
4. **Review traces** - Check platform dashboard for optimization opportunities
5. **Start with common workflows** - Build skill library for repeated tasks

## Production Readiness

**✅ Ready for production IF:**

- You have clear, well-tested instructions
- Tasks are operational (not simple Q&A)
- You monitor and optimize prompts
- You use skills for repeated workflows

**⚠️ Not recommended for:**

- Simple customer support (use direct MCP)
- Mission-critical real-time operations
- Tasks requiring <1s response time

## References

- [Anthropic: Code Execution with MCP](https://www.anthropic.com/engineering/code-execution-with-mcp)
- [Agency Swarm Documentation](https://agency-swarm.ai)
- [Notion MCP Server](https://developers.notion.com/docs/get-started-with-mcp)
- [Google Drive MCP Server](https://github.com/isaacphi/mcp-gdrive)

## Contributing

This is a reference implementation of the Code Execution Pattern. Improvements welcome:

1. Better prompting strategies
2. More efficient skill suggestions
3. Additional MCP server integrations
4. Performance optimizations

## License

MIT

---

**Built with [Agency Swarm](https://agency-swarm.ai) implementing [Anthropic's Code Execution Pattern](https://www.anthropic.com/engineering/code-execution-with-mcp)**



================================================
FILE: agency.py
================================================
from dotenv import load_dotenv
from agency_swarm import Agency

from sales_ops import sales_ops
from sales_ops_direct_mcp import sales_ops_direct_mcp

import asyncio

load_dotenv()

# do not remove this method, it is used in the main.py file to deploy the agency (it has to be a method)
def create_agency(load_threads_callback=None):
    agency = Agency(
        sales_ops, sales_ops_direct_mcp,
        communication_flows=[],
        name="SalesOpsAgency",
        shared_instructions="shared_instructions.md",
        load_threads_callback=load_threads_callback,
    )

    return agency

if __name__ == "__main__":
    agency = create_agency()

    # test 1 message
    # async def main():
    #     response = await agency.get_response("Hello, how are you?")
    #     print(response)
    # asyncio.run(main())

    # run in terminal
    agency.terminal_demo()


================================================
FILE: AGENTS.md
================================================
# Agent Creator Agent Instructions

Agency Swarm is the framework built on the OpenAI Agents SDK. It allows anyone to create a collaborative swarm of agents (Agencies), each with distinct roles and capabilities. Your primary role is to architect tools and agents that fulfill specific needs within the agency. Helpful references for building agents include:

- Official docs: <https://agency-swarm.ai>
- Source code: <https://github.com/VRSEN/agency-swarm>
- Examples repository: <https://github.com/VRSEN/agency-swarm/tree/main/examples>

Fetch these resources to familiarize yourself with the framework as needed.

The following steps outline how to build agents from a single prompt:

1. **PRD Creation:** Gather information to draft a Product Requirements Document (PRD) for the agency.
2. **Folder Structure and Template Creation:** Create the Agent Templates for each agent using the CLI Commands provided below.
3. **Tool Development:** Develop each tool and place it in the correct agent's tools folder, ensuring it is robust and ready for production environments.
4. **Agent Creation:** Create agent classes and instructions for each agent, ensuring correct folder structure.
5. **Agency Creation:** Create the agency class in the agency folder, properly defining the communication flows between the agents.
6. **Testing:** Test each tool for the agency, and the agency itself, to ensure they are working as expected.
7. **Iteration:** Repeat the above steps as instructed by the user, until the agency performs consistently to the user's satisfaction.

You will find a detailed guide for each of the steps below. Read this entire file first before proceeding.

# Step 1: PRD Creation

First, ask the user to provide all necessary details:

- Agency Name
- Purpose (a high-level description of what the agency aims to achieve, its target market, and its value proposition)
- Communication Flows (between agents and from agents to user)
- Agents (for each agent: name, role, tools with descriptions)

Once you have gathered all details, create the file `./prd.txt` using the following template:

```md
# [Agency Name]

---

- **Purpose:** [A high-level description of what the agency aims to achieve, its target market, and the value it offers to its clients.]
- **Communication Flows:**
  - **Between Agents:**
    - [Description of the communication protocols and flows between different agents within the agency, including any shared resources or data.]
    - **Example Flow:**
      - **Agent A -> Agent B:** [Description of the interaction, including trigger conditions and expected outcomes.]
      - **Agent B -> Agent C:** [Description of the interaction, including trigger conditions and expected outcomes.]
  - **Agent to User Communication:** [Description of how agents will communicate with end-users, including any user interfaces or channels used.]

---

## Agent Name

### **Role within the Agency**

[Description of the agent's specific role and responsibilities within the agency.]

### Tools

- **ToolName:**
  - **Description**: [Description on what this tool should do and how it will be used]
  - **Inputs**:
    - [name] (type) - description
  - **Validation**:
    - [Condition] - description
  - **Core Functions:** [List of the main functions the tool must perform.]
  - **APIs**: [List of APIs the tool will use]
  - **Output**: [Description of the expected output of the tool. Output must be a string or a JSON object.]

---

...repeat for each agent
```

After the user provides the requested details, proceed to drafting the PRD file right away. Provide file path to the PRD file in the response and ask the user to edit it if needed. Once approved, read the PRD file contents again and proceed to the next step.

### Best Practices

- **Each tool must perform a specific realistic task**: Do not create tools that do not correspond to real world tasks. Each tool should focus on a specific action a human typically would perform. For example, "FetchLeadsFromInstagram" is a valid tool, but "OptimizeEmailSubject" is not. Optimizing email subjects is not a real task. Typically, a tool either: 1) connects to an external API, 2) performs a specific action on local files, 3) uses AI inside.
- **4-16 Tools Per Agent**: Each agent should have between 4 and 16 tools. Avoid breaking down the agency into too many agents, unless their responsibilities are significantly different, or the user has requested it. **Avoid creating tools that the user has not requested.** If it's unclear what tools the agent will need, ask the user for clarification or continue without them. Note: Each MCP typically contains multiple tools. Count each as 5-10 tools, depending on MCP complexity and capabilities.
- **Ask for API keys before creating any tools**: If a tool requires an API key or access token, ask the user to add it to the .env file. Do not proceed with creating the tool until the API key is added, otherwise you will not be able to test them.

# Step 2: Folder Structure and Template Creation

After creating the PRD file, setup the folder structure for each agent.

The basic folder structure is already created for you:

```
├── example_agent/
│   ├── __init__.py
│   ├── example_agent.py
│   ├── instructions.md
│   ├── files/
│   └── tools/
│       ├── ToolName.py
│       ├── ToolName2.py
│       ├── ToolName3.py
│       ├── ...
├── example_agent2/
│   ├── __init__.py
│   ├── example_agent2.py
│   ├── instructions.md
│   ├── files/
│   └── tools/
│       ├── ToolName.py
│       ├── ToolName2.py
│       ├── ToolName3.py
│       ├── ...
├── agency.py
├── shared_instructions.md
├── requirements.txt
├── .env
└──...
```

**Folder Structure Rules**:

- Agency folder must be named in lowercase, with underscores instead of spaces.
- Each agency and agent has its own dedicated folder.
- Within each agent folder:

  - A 'tools' folder contains all tools for that agent.
  - An 'instructions.md' file provides agent-specific instructions.
  - An '**init**.py' file contains the import of the agent.

- Tool Import Process:

  - Create a file in the 'tools' folder with the same name as the tool class.
  - Tools are automatically imported to the agent class.
  - All new requirements must be added to the requirements.txt file.

- Agency Configuration:
  - The 'agency.py' file is the main file where all new agents are imported.
  - When creating a new agency folder, use descriptive names, like for example: marketing_agency, development_agency, etc.
  - Create a `.env` file in the root folder and add a placeholder for `OPENAI_API_KEY` and any other API keys that are required by the tools for the user to fill in.

Follow this folder structure when further creating or modifying any files. Replace example_agent folders with the actual agents when creating agents for the first time.

# Step 3: Tool Creation

Tools are the specific actions that agents can perform. They are defined using pydantic, which provides a convenient interface and automatic type validation. Below is a complete example of a tool file:

```python
# MyCustomTool.py
from agency_swarm.tools import BaseTool
from pydantic import Field
import os
from dotenv import load_dotenv

load_dotenv() # always load the environment variables

class MyCustomTool(BaseTool):
    """
    A brief description of what the custom tool does.
    The docstring should clearly explain the tool's purpose and functionality.
    It will be used by the agent to determine when to use this tool.
    """
    # Define the fields with descriptions using Pydantic Field
    example_field: str = Field(
        ..., description="Description of the example field, explaining its purpose and usage for the Agent."
    )

    def run(self):
        """
        The implementation of the run method, where the tool's main functionality is executed.
        This method should utilize the fields defined above to perform the task.
        """
        # Your custom tool logic goes here
        # Example:
        # account_id = "MY_ACCOUNT_ID"
        # api_key = os.getenv("MY_API_KEY") # or access_token = os.getenv("MY_ACCESS_TOKEN")
        # do_something(self.example_field, api_key, account_id)

        # Return the result of the tool's operation as a string
        return "Result of MyCustomTool operation"

if __name__ == "__main__":
    tool = MyCustomTool(example_field="example value")
    print(tool.run())
```

Remember, each tool code snippet you create must be IMMIDIATELY ready to use by the user. It must not contain any mocks, placeholders or hypothetical examples.

### Best Practices

- **Use Python Packages**: Prefer to use various API wrapper packages and SDKs available on pip, rather than calling these APIs directly using requests.
- **Comments**: The code should be well commented, with clear step-by-step explanations of the code. (Eg. "# Step 1: Do something", "# Step 2: Do something else", etc.)
- **Code Reliability**: Write actual functional code, without placeholders or hypothetical examples.
- **NEVER include API keys as tool inputs**: If a tool needs an API key or access token, always retrieve it from environment variables using the `os` package inside the `run` method. Do not define API keys or tokens as input fields for the tool.
- **Use global variables for constants**: If a tool requires a constant global variable, that does not change from use to use, (for example, ad_account_id, pull_request_id, etc.), define them as constant global variables above the tool class, instead of inside Pydantic `Field`.
- **Add a test case at the bottom of the file**: Add a test case for each tool in if **name** == "**main**": block. It will be used to test the tool later.
- **Avoid Redundant Logic**: Do not create tools that introduce redundant logic inside. For example, when creating a SQL Database Agent, do not create a tool `AnalyzeDatabase` that performs some complicated math analysis inside. Instead, simply connect an agent to the database with a `QueryDatabase` tool, and let the agent perform the analysis.
- **Keep Tools Single-Purpose**: Create atomic self-contained tools that perform specific tasks or operations. The agent should then be able to combine these tools autonomously for more complex workflows. In the SQL Database Agent example above, it makes sense to also add a `GetMetadata` tool, so the agent can use that tool before using the `QueryDatabase` tool.

### Agency Context (Shared State)

Agency context lets your tools and agents share data without passing it in conversation messages.

```python
from agency_swarm.tools import BaseTool

class MyTool(BaseTool):
    value: str = Field(..., description="The value to store in the context")
    def run(self):
        self._context.set("my_key", self.value)
        data = self._context.get("my_key", "default")
        return data
```

Use agency context for:

- Large data structures that are expensive to pass between agents
- Maintaining state across multiple tool calls
- Sharing data among tools and agents

Best practices:

- Use descriptive keys to avoid conflicts
- Provide default values when calling `get`
- Clean up unneeded data to keep the context small

### MCP Integration

Alternatively to creating custom tools, you can use special MCP servers which already contain predefined tools. In this case, you don't need to create custom tool files for the same functionality or add them to the PRD. You can use MCPs interchangeably with custom tools

```python
from agents.mcp import MCPServerStdio

filesystem_server = MCPServerStdio(
    # This name determines how the agent accesses the tools (e.g., Filesystem_Server.list_files)
    name="Filesystem_Server",
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", "."],
    },
    cache_tools_list=True
)
# Attach this server to your Agent via the mcp_servers list:
# my_agent = Agent(..., mcp_servers=[sse_server])
# Reference: https://agency-swarm.ai/core-framework/tools/mcp-integration#step-2-define-sse-server-connection-optional
```

For remote MCP servers, you can use the `MCPServerSse` or `MCPServerStreamableHttp` classes. See documentation for more details.

**Important**: Prefer creating MCP servers to connect to common platforms like Notion or Hubspot over custom tools. To find the best suited MCP servers, use web search.

### Web Search Tool (Built-in)

Agency Swarm has a built-in tool for web searchs. If the agent requires web searchs, you can simply include it in the agent's tools list.

```python
from agents.tool import WebSearchTool

tools = [WebSearchTool()]
```

# Step 4: Agent Creation

To create an agent:

1. **Create an agent module.**:

   ```python
   from agents import ModelSettings
   from agency_swarm import Agent
   from openai.types.shared import Reasoning

   ceo = Agent(
       name="CEO",
       description="Responsible for client communication, task planning and management.",
       instructions="./instructions.md",
       tools_folder="./tools",
       files_folder="./files",
       model="gpt-5",
       model_settings=ModelSettings(
           reasoning=Reasoning(
               effort="medium",
               summary="auto",
           ),
       ),
   )
   ```

   - **name**: The agent's name, reflecting its role.
   - **description**: A brief summary of the agent's responsibilities.
   - **instructions**: Path to a markdown file containing detailed instructions for the agent.
   - **model**: The model to use for the agent. **Use gpt-5 by default**, which is already available.
   - **tools_folder**: Folder containing the tools for the agent. Tool modules are automatically imported. Each tool class must be named the same as the tool file. For example, if the tool class is named `MyTool`, the tool file must be named `MyTool.py`.
   - **Other Parameters**: Additional settings like `model_settings` or persistence callbacks.

   Make sure to create a separate folder for each agent, as described in the folder structure above. After creating the agent, you need to import it into the agency.py file.

2. **Create an `instructions.md` file in the agent's folder.**

   Each agent also needs to have an `instructions.md` file, which is the system prompt for the agent. Inside those instructions, you need to define the following:

   - **Agent Role**: A description of the role of the agent.
   - **Goals**: A list of goals that the agent should achieve, aligned with the agency's mission.
   - **Process Workflow**: A step by step guide on how the agent should perform its tasks. Each step must be aligned with the other agents in the agency, and with the tools available to this agent.

   Use the following template for the instructions.md file:

   ```md
   # Role

   You are **[insert role, e.g., "a helpful expert" or "a creative storyteller".]**

   # Instructions

   # [Task Name]

   **[Provide a step-by-step instructions process on how this task should be performed. Use a numbered list.]**

   [...repeat for each task]

   # Additional Notes

   - **[Specify any additional notes here, if any. Use bullet points if needed.]**
   ```

### Best Practices

- **Start Simple**: Use concise, verb-driven instructions.
- **Be Specific**: Explicitly state desired outputs and formats.
- **Provide Examples**: Include concrete examples of expected behavior and tool usage.
- **Use Positive Instructions**: Phrase steps as "Do this" rather than "Don't do that".
- **Integrate Tools in Steps**: Show exactly when and how to use each tool in the workflow.
- **Use Variables**: Parameterize dynamic values with placeholders for clarity.
- **Test Continuously**: Refine instructions based on actual test results and feedback.
- **Avoid Speculation**: Be conscientious when creating instructions—avoid guessing or making unsupported assumptions. If certain information is not available, simply leave it blank so the user can fill it in.
- **Performance Targets**: Include response time or quality targets where appropriate.
- **Output Formats**: Specify exact output schemas or formats for agent responses.
- **Preserve Context**: Ensure instructions maintain message thread and context for multi-turn conversations.
- **Log Decisions**: Instruct agents to log important decisions for audit and traceability.

# Step 5: Agency Creation

Agencies are collections of agents that work together to achieve a common goal. They are defined in the `agency.py` file, which already exists in the root folder.

1. **Import the agents into the `agency.py` file.**

   ```python
   from dotenv import load_dotenv
   from agency_swarm import Agency
   from ceo import ceo
   from developer import developer
   from virtual_assistant import virtual_assistant

   load_dotenv()

   # do not remove this method, it is used in the main.py file to deploy the agency (it has to be a method)
   def create_agency(load_threads_callback=None):
       agency = Agency(
           ceo,
           communication_flows=[
               (ceo, developer),
               (ceo, virtual_assistant),
               (developer, virtual_assistant),
           ],
           shared_instructions="shared_instructions.md",
       )
       return agency

   if __name__ == "__main__":
       agency = create_agency()
       agency.terminal_demo()
   ```

   Agency must export a create_agency method, which is used for deployment.

   The first argument is the entry point for user communication. The communication flows are defined in the `communication_flows` parameter.

   **A Note on Communication Flows**:

   Communication flows are directional. In the `communication_flows` parameter above, the agent on the left can initiate conversations with the agent on the right.

2. **Define the `shared_instructions.md` file.**

   Shared instructions is a file that contains shared instructions for all agents in the agency. Typically, this file might contain information about the business, ICP, target audience, environment, etc. If user has not provided any information, create a template with headings but leave the content blank for the user to fill in.

# Step 6: Testing

The final step is to test each tool and the agency itself, to ensure they are working as expected.

1. First, install the dependencies for the agency using the following command:

   ```bash
   pip install -r requirements.txt
   ```

2. Then, run each tool file in the tools folder that you created, to ensure they are working as expected.

   ```bash
   python agent_name/tools/tool_name.py
   ```

   If any of the tools return an error, you need to fix the code in the tool file.

3. Once all tools are working as expected, you can test the agency by running the following command:

   ```bash
   python agency.py
   ```

   If the terminal demo runs successfully, you have successfully created an agency that works as expected.

**Important**: Please do not stop on this step until all new tools and agents have been tested and are working as expected. Do not ask for confirmation or wait for the user to respond. Just keep iterating until the agency performs as expected.

# Step 7: Iteration

Repeat the above steps as instructed by the user, until the agency performs consistently to the user's satisfaction. First, adjust the tools, then adjust the agents and instructions, then test again. Make sure to repeat each step accordingly.

If the user is not starting their agency from scratch, you can start from one of the steps above accordingly.

# Final Notes

1. NEVER output code snippets or file contents in the chat. Always create or modify the actual files in the file system. If you're unsure about a file's location or content, check the current folder structure and file contents before proceeding. If you find yourself about to output code in the chat, STOP and reconsider your approach.
2. Never create files with sample snippets, hypothetical examples or placeholders. When creating custom tools, you must create production-ready functional code.
3. Ensure all tools are properly tested before submitting your work to the user.
4. Follow the specified file creation order rigorously: 1. requirements.txt, 2. tools, 3. agents, 4. instructions.md. 5. agency.py.
5. You can find more documentation in the ./.claude/agents/\*\* for each part of the agent building process accordingly.
6. Additionally, there is an .cursor/commands/add-mcp.md file that contains the instructions for adding MCP servers your agents.
7. Before strating to build the new agency, remove example agents.



================================================
FILE: CLAUDE.md
================================================
# Agency Builder

You are a specialized agent that coordinates specialized sub-agents to build production-ready Agency Swarm v1.0.0 agencies.

## Background

Agency Swarm is an open-source framework designed for orchestrating and managing multiple AI agents, built upon the OpenAI Assistants API. Its primary purpose is to facilitate the creation of "AI agencies" or "swarms" where multiple AI agents with distinct roles and capabilities can collaborate to automate complex workflows and tasks. 

### A Note on Communication Flow Patterns

In Agency Swarm, communication flows are uniform, meaning you can define them in any way you want. Below are some examples:

#### Orchestrator-Workers (Most Common)
```python
agency = Agency(
    ceo,  # Entry point for user communication
    communication_flows=[
        (ceo, worker1),
        (ceo, worker2),  
        (ceo, worker3),
    ],
    shared_instructions="agency_manifesto.md",
)
```

#### Sequential Pipeline (handoffs)
```python
from agency_swarm.tools.send_message import SendMessageHandoff

# Each agent needs SendMessageHandoff as their send_message_tool_class
agent1 = Agent(..., send_message_tool_class=SendMessageHandoff)
agent2 = Agent(..., send_message_tool_class=SendMessageHandoff)

agency = Agency(
    agent1,
    communication_flows=[
        (agent1, agent2),
        (agent2, agent3),
    ],
    shared_instructions="agency_manifesto.md",
)
```

#### Collaborative Network
```python
agency = Agency(
    ceo,
    communication_flows=[
        (ceo, developer),
        (ceo, designer),
        (developer, designer),
    ],
    shared_instructions="agency_manifesto.md",
)
```

See documentation for more details.

## Available Sub-Agents

- **api-researcher**: Researches MCP servers and APIs, saves docs locally
- **prd-creator**: Transforms concepts into PRDs using saved API docs
- **agent-creator**: Creates complete agent modules with folder structure
- **tools-creator**: Implements tools prioritizing MCP servers over custom APIs
- **instructions-writer**: Write optimized instructions using prompt engineering best practices
- **qa-tester**: Test agents with actual interactions and tool validation

## Orchestration Responsibilities

1. **User Clarification**: Ask questions one at a time when idea is vague
2. **Research Delegation**: Launch api-researcher to find MCP servers/APIs
3. **Documentation Management**: Download Agency Swarm docs if needed
4. **Parallel Agent Creation**: Launch agent-creator, tools-creator, and instructions-writer simultaneously
5. **API Key Collection**: ALWAYS ask for API keys before testing
6. **Issue Escalation**: Relay agent escalations to user
7. **Test Result Routing**: Pass test failure files to relevant agents
8. **Communication Flow Decisions**: Determine agent communication patterns
9. **Workflow Updates**: Update this file when improvements discovered

## Workflows

### 1. When user has vague idea:
1. Ask clarifying questions to understand:
   - Core purpose and goals of the agency
   - Expected user interactions
   - Data sources/APIs they want to use
2. **WAIT FOR USER FEEDBACK** before proceeding to next steps
3. Launch api-researcher with concept → saves to `agency_name/api_docs.md` with API key instructions
4. Launch prd-creator with concept + API docs path → returns PRD path
5. **CRITICAL: Present PRD to user for confirmation**
   - Show PRD summary with agent count and tool distribution
   - Ask: "Does this architecture look good? Should we proceed?"
   - **WAIT FOR USER APPROVAL** before continuing
6. **Collect API keys BEFORE development** (with instructions from api-researcher):
   - OPENAI_API_KEY (required) - Show instructions how to get it
   - Tool-specific keys - Show instructions for each
   - **WAIT FOR USER TO PROVIDE ALL KEYS**
7. **PHASED EXECUTION**:
   - **Phase 1** (Parallel): Launch simultaneously:
     - agent-creator with PRD → creates agent modules and folders
     - instructions-writer with PRD → creates instructions.md files
   - **Phase 2** (After Phase 1 completes):
     - tools-creator with PRD + API docs + API keys → implements and tests tools
8. Launch qa-tester → sends 5 test queries, returns results + improvement suggestions
9. **Iteration based on QA results**:
   - Read `qa_test_results.md` for specific suggestions
   - Prioritize top 3 improvements from qa-tester
   - Delegate with specific instructions:
     - Instruction improvements → instructions-writer with exact changes
     - Tool fixes → tools-creator with specific issues to fix
     - Communication flow → update agency.py directly
   - Track changes made for each iteration
10. Re-run qa-tester with same 5 queries to verify improvements
11. Continue iterations until:
    - All 5 test queries pass
    - Response quality score ≥8/10
    - No critical issues remain

### 2. When user has detailed specs:
1. Launch api-researcher if APIs mentioned → saves docs with API key instructions
2. Create PRD from specs if not provided
3. **Get user confirmation on architecture**
4. **Collect all API keys upfront** (with instructions)
5. **PHASED EXECUTION**:
   - Phase 1: agent-creator + instructions-writer (parallel)
   - Phase 2: tools-creator (after Phase 1)
6. Launch qa-tester with 5 test queries
7. Iterate based on qa-tester suggestions

### 3. When adding new agent to existing agency:
1. Update PRD with new agent specs (follow 4-16 tools rule)
2. **Get user confirmation on updated PRD**
3. Research new APIs if needed via api-researcher
4. **Collect any new API keys** (with instructions)
5. **PHASED EXECUTION** for new agent:
   - Phase 1: agent-creator + instructions-writer
   - Phase 2: tools-creator (tests each tool)
6. Update agency.py with new communication flows
7. Launch qa-tester to validate integration

### 4. When refining existing agency:
1. Launch qa-tester → creates test results with improvement suggestions
2. Review suggestions and prioritize top issues
3. Pass specific fixes to agents:
   - instructions-writer: "Update agent X instructions, line Y"
   - tools-creator: "Fix tool Z error handling"
4. Re-test with same queries to track improvement
5. Document improvement metrics after each iteration

## Key Patterns

- **Phased Execution**: agent-creator + instructions-writer first, THEN tools-creator
- **PRD Confirmation**: Always get user approval before development
- **API Keys First**: Collect ALL keys with instructions before any development
- **File Ownership**: Each agent owns specific files to prevent conflicts
- **MCP Priority**: Always prefer MCP servers over custom tools
- **Tool Testing**: tools-creator tests each tool individually
- **QA Testing**: qa-tester sends 5 example queries and suggests improvements
- **Iteration**: Use qa-tester feedback to improve agents
- **Progress Tracking**: Use TodoWrite extensively

## Context for Sub-Agents

When calling sub-agents, always provide:
- Clear task description
- Relevant file paths (PRD, API docs, test results)
- Reference to online Agency Swarm docs: https://agency-swarm.ai
- Expected output format (usually file path + summary)
- Framework version (Agency Swarm v1.0.0)
- Communication flow pattern for the agency
- For phased execution: Which phase we're in
- API keys already collected (don't ask agents to get them)
- For iterations: Specific improvements needed from qa-tester feedback


================================================
FILE: Dockerfile
================================================
FROM python:3.13-slim

ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PATH="/root/.local/bin:${PATH}"

WORKDIR /app

# Install Node.js for MCPs
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        git \
        curl \
        ca-certificates \
        gnupg && \
    mkdir -p /etc/apt/keyrings && \
    curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg && \
    echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main" | tee /etc/apt/sources.list.d/nodesource.list && \
    apt-get update && \
    apt-get install -y --no-install-recommends nodejs && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

COPY . .
CMD python -u main.py


================================================
FILE: main.py
================================================
# NOTE: Do not modify this file. This file is managed by the deployment and integration system.

import logging
from dotenv import load_dotenv

load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)

from agency import create_agency
from agency_swarm.integrations.fastapi import run_fastapi


if __name__ == "__main__":
    run_fastapi(
        agencies={
            # you must export your create agency function here
            "my-agency": create_agency,
        },
        port=8080,
        enable_logging=True
    )


================================================
FILE: pyproject.toml
================================================
# only listed properties are used by the system

[project]
name = "my-agency"
description = "My awesome agency"
version = "0.0.1"
license = { text = "MIT" }
keywords = ["agency-swarm", "agencii"]

[tool.template]
version = "0.0.2"


================================================
FILE: requirements.txt
================================================
agency-swarm[fastapi]>=1.4.0
agency-swarm[jupyter]>=1.4.0
uvicorn
nest_asyncio



================================================
FILE: shared_instructions.md
================================================
[Empty file]


================================================
FILE: .dockerignore
================================================
# Git
.git/
.gitignore

# IDE and Cursor
.claude/
.cursor/

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
*.egg-info/

# Virtual environments
venv/
env/
.venv/

# IDE
.vscode/
.idea/

# Environment
.env
.env.*
!.env.template

# OS
.DS_Store
Thumbs.db

# Docker
Dockerfile
.dockerignore

# Logs
*.log



================================================
FILE: .env.template
================================================
OPENAI_API_KEY=
GDRIVE_CREDENTIALS_JSON=


================================================
FILE: mnt/skills/.gitkeep
================================================
# This file ensures the skills directory is tracked by git
# Skills (Python files) will be added here over time as the agent learns




================================================
FILE: sales_ops/__init__.py
================================================
from .sales_ops import sales_ops

__all__ = ["sales_ops"]




================================================
FILE: sales_ops/instructions.md
================================================
# Role

You are **a Sales Operations Specialist responsible for automating and streamlining operational tasks for the team using available tools and creating reusable workflows.**

# Goals

- **Improve team efficiency by automating repetitive operational tasks**
- **Build a library of reusable skills (functions) for common workflows**
- **Minimize manual work by combining tools intelligently**

# Context

- Part of: Sales Operations Agency
- Works with: Team members requesting operational tasks
- Used for: Automating data retrieval, reporting, cross-platform operations (Notion + Google Drive)

# Instructions

## Task Execution Process

When you receive a task request, follow this systematic approach:

### 1. Discover Existing Skills

**Use PersistentShellTool to check for existing skills:**

```bash
ls -la ./mnt/skills/
```

Look for Python files that match the task description. Skills are reusable functions for common workflows.

### 2. Evaluate Skill Match

If a skill exists that matches the task:

- Read the skill file to understand its purpose and parameters
- Use IPythonInterpreter to load and execute the skill
- Proceed to step 5 (Output)

### 3. Build Solution from Tools (No Matching Skill Found)

If no existing skill matches, build a solution using MCP tools:

**Step 3.1: Identify Required Tools**

Based on the task, determine which tools you need:

- Notion operations? → Check `servers/notion/`
- Google Drive/Sheets operations? → Check `servers/gdrive/`
- Both platforms? → Use tools from both servers

**Step 3.2: Read ONLY the Necessary Tool Files**

Use PersistentShellTool to read only the specific tool files you need:

```bash
# Example: If you need to search Notion and update a Google Sheet
cat ./servers/notion/search.py
cat ./servers/gdrive/update_cell.py
```

**DO NOT** read any other tool, readme, or server files to avoid extra token consumption. Only read what you need for the specific task.

**Step 3.3: Combine Tools in IPythonInterpreter**

Use IPythonInterpreter to write and execute the solution (supports async/await):

```python
# Step 1: Import only the tools you need
from servers.notion import search
from servers.gdrive import update_cell

# Step 2: Execute the workflow with await (top-level await is supported)
results = await search(query="Q4 revenue", query_type="internal")

# Step 3: Process and export results
# ... process results ...
await update_cell(fileId="sheet_id", range="A1", value="data")
```

**Note**: The IPythonInterpreter has built-in async support. You do NOT need to call `nest_asyncio.apply()` - just use `await` directly.

### 4. Suggest New Skills

After completing a task, analyze the workflow and suggest reusable skills:

**Criteria for suggesting a skill:**

- Task will likely be repeated in the future
- Combines 2+ tools or operations
- Has clear, parameterizable inputs
- Provides consistent output format

# Output Format

Always provide:

1. **Task Summary** - What was completed, tools used, results
2. **Efficiency Suggestions** - Recommended skills to create for future efficiency. Don't make it too technical. Don't output the code, only the skill name, and purpose.

Use clear, concise language. Include specific file IDs, URLs, and data where relevant.

# Additional Notes

- Perform as few tool calls as possible in order to complete the task.
- DO NOT read files like server.py, README.md, etc. to avoid extra token consumption.



================================================
FILE: sales_ops/sales_ops.py
================================================
from agency_swarm import Agent
from agency_swarm.tools import PersistentShellTool, IPythonInterpreter
from openai.types.shared import Reasoning
from agents import ModelSettings

sales_ops = Agent(
    name="SalesOps",
    description="Sales operations agent equipped with Python interpreter and shell tools for automating operational tasks using Notion and Google Drive MCP tools.",
    instructions="./instructions.md",
    tools_folder="./tools",
    files_folder="./files",
    tools=[PersistentShellTool, IPythonInterpreter],
    model="gpt-5",
    model_settings=ModelSettings(
        reasoning=Reasoning(
            effort="medium",
            summary="auto",
        ),
    ),
)




================================================
FILE: sales_ops_direct_mcp/__init__.py
================================================
from .sales_ops_direct_mcp import sales_ops_direct_mcp

__all__ = ["sales_ops_direct_mcp"]




================================================
FILE: sales_ops_direct_mcp/instructions.md
================================================
# Role

You are a **Sales Operations Specialist** with direct access to Google Drive and Notion through MCP integration.

# Goals

- Efficiently manage and organize sales data across Google Drive and Notion
- Provide quick access to sales documents, spreadsheets, and databases
- Automate data retrieval and updates for sales operations tasks

# Process

## Accessing Sales Data from Google Drive

1. Use the **GoogleDrive** MCP server to search for files using `gdrive_search`
2. Read file contents with `gdrive_read_file` (supports Docs as Markdown, Sheets as CSV)
3. For Google Sheets operations:
   - Read data using `gsheets_read` with flexible range options
   - Update cell values using `gsheets_update_cell`

## Working with Notion Databases

1. Use the **Notion** MCP server to access workspace content
2. Fetch pages and databases using `notion-fetch`
3. Create new pages or database entries using `notion-create-pages`
4. Update existing pages with `notion-update-page`
5. Search across the workspace using `notion-search`

## Data Synchronization Tasks

1. Identify the data source (Google Drive or Notion)
2. Use appropriate MCP tools to fetch the required data
3. Process and format the data as needed
4. Update target systems using available MCP tools
5. Confirm successful data operations

# Output Format

- Provide clear, concise responses about data operations
- Include file IDs, page IDs, or relevant identifiers in responses
- Summarize data changes or updates performed

# Additional Notes

- Google Drive tools automatically export Google Workspace files to accessible formats



================================================
FILE: sales_ops_direct_mcp/sales_ops_direct_mcp.py
================================================
from agency_swarm import Agent
import sys
from pathlib import Path
from openai.types.shared import Reasoning
from agents import ModelSettings

# Add parent directory to path to import server configurations
parent_dir = Path(__file__).parent.parent
sys.path.insert(0, str(parent_dir))

from servers.gdrive.server import get_server as get_gdrive_server
from servers.notion.server import get_server as get_notion_server

# Get the MCP server instances
gdrive_server = get_gdrive_server()
notion_server = get_notion_server()

sales_ops_direct_mcp = Agent(
    name="SalesOpsDirectMCP",
    description="Sales operations agent with direct MCP integration for advanced capabilities.",
    instructions="./instructions.md",
    tools_folder="./tools",
    files_folder="./files",
    model="gpt-5",
    mcp_servers=[gdrive_server, notion_server],
    model_settings=ModelSettings(
        reasoning=Reasoning(
            effort="medium",
            summary="auto",
        ),
    ),
)


if __name__ == "__main__":
    import asyncio

    async def test_mcp_integration():
        """Test MCP server integration"""
        print("Testing MCP server integration for SalesOpsDirectMCP agent...\n")
        
        # Test Google Drive MCP Server
        print("=" * 60)
        print("Google Drive MCP Server")
        print("=" * 60)
        try:
            await gdrive_server.connect()
            tools_result = await gdrive_server.list_tools()
            tools = tools_result.tools if hasattr(tools_result, 'tools') else tools_result
            print(f"✓ Connected successfully")
            print(f"✓ Found {len(tools)} tools:\n")
            for tool in tools:
                print(f"  • {tool.name}")
                if hasattr(tool, 'description'):
                    print(f"    {tool.description}")
        except Exception as e:
            print(f"✗ Error connecting to Google Drive MCP: {e}")
        
        print("\n" + "=" * 60)
        print("Notion MCP Server")
        print("=" * 60)
        try:
            await notion_server.connect()
            tools_result = await notion_server.list_tools()
            tools = tools_result.tools if hasattr(tools_result, 'tools') else tools_result
            print(f"✓ Connected successfully")
            print(f"✓ Found {len(tools)} tools:\n")
            for tool in tools:
                print(f"  • {tool.name}")
                if hasattr(tool, 'description'):
                    print(f"    {tool.description}")
        except Exception as e:
            print(f"✗ Error connecting to Notion MCP: {e}")
        
        print("\n" + "=" * 60)
        print("Agent Response Test")
        print("=" * 60)
        try:
            result = await sales_ops_direct_mcp.get_response(
                "List all the tools you have access to from Google Drive and Notion."
            )
            print(f"\n{result.final_output}")
        except Exception as e:
            print(f"✗ Error getting agent response: {e}")

    asyncio.run(test_mcp_integration())




================================================
FILE: servers/gdrive/__init__.py
================================================
"""
Google Drive MCP Tools

Progressive disclosure pattern - import only what you need.
See: https://www.anthropic.com/engineering/code-execution-with-mcp
"""

# Server management
from .server import get_server, ensure_connected, call_tool

# Individual tools
from .gdrive_search import gdrive_search
from .gdrive_read_file import gdrive_read_file
from .gdrive_update_cell import gdrive_update_cell
from .gdrive_read_sheet import gdrive_read_sheet

__all__ = [
    # Server
    "get_server",
    "ensure_connected",
    "call_tool",
    # Tools
    "gdrive_search",
    "gdrive_read_file",
    "gdrive_update_cell",
    "gdrive_read_sheet",
]




================================================
FILE: servers/gdrive/gdrive_read_file.py
================================================
from .server import call_tool


async def gdrive_read_file(fileId: str) -> str:
    """
    Read contents of a file from Google Drive

    Args:
        fileId: ID of the file to read

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"fileId": fileId}

    # Call the MCP tool
    return await call_tool("gdrive_read_file", arguments)


# Test - run with: python ./servers/gdrive/gdrive_read_file.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing gdrive_read_file...")
        print("Note: This requires Google Drive authentication")
        print("Skipping actual test to avoid authentication requirements")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/gdrive/gdrive_read_sheet.py
================================================
from typing import Optional, List
from .server import call_tool


async def gdrive_read_sheet(
    spreadsheetId: str,
    ranges: Optional[List[str]] = None,
    sheetId: Optional[int] = None
) -> str:
    """
    Read data from a Google Spreadsheet with flexible options for ranges and formatting

    Args:
        spreadsheetId: The ID of the spreadsheet to read
        ranges: Optional array of A1 notation ranges like ['Sheet1!A1:B10']. If not provided, reads entire sheet.
        sheetId: Optional specific sheet ID to read. If not provided with ranges, reads first sheet.

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"spreadsheetId": spreadsheetId}

    # Add optional params only if provided
    if ranges:
        arguments["ranges"] = ranges
    if sheetId is not None:
        arguments["sheetId"] = sheetId

    # Call the MCP tool
    return await call_tool("gsheets_read", arguments)


# Test - run with: python ./servers/gdrive/gdrive_read_sheet.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing gdrive_read_sheet...")
        print("Note: This requires Google Drive authentication")
        print("Skipping actual test to avoid authentication requirements")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/gdrive/gdrive_search.py
================================================
from typing import Optional
from .server import call_tool


async def gdrive_search(
    query: str,
    pageToken: Optional[str] = None,
    pageSize: Optional[int] = None
) -> str:
    """
    Search for files in Google Drive

    Args:
        query: Search query
        pageToken: Token for the next page of results
        pageSize: Number of results per page (max 100)

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"query": query}

    # Add optional params only if provided
    if pageToken:
        arguments["pageToken"] = pageToken
    if pageSize:
        arguments["pageSize"] = pageSize

    # Call the MCP tool
    return await call_tool("gdrive_search", arguments)


# Test - run with: python ./servers/gdrive/gdrive_search.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing gdrive_search...")
        print("Note: This requires Google Drive authentication")
        print("Skipping actual test to avoid authentication requirements")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/gdrive/gdrive_update_cell.py
================================================
from .server import call_tool


async def gdrive_update_cell(
    fileId: str,
    range: str,
    value: str
) -> str:
    """
    Update a cell value in a Google Spreadsheet

    Args:
        fileId: ID of the spreadsheet
        range: Cell range in A1 notation (e.g. 'Sheet1!A1')
        value: New cell value

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {
        "fileId": fileId,
        "range": range,
        "value": value
    }

    # Call the MCP tool
    return await call_tool("gsheets_update_cell", arguments)


# Test - run with: python ./servers/gdrive/gdrive_update_cell.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing gdrive_update_cell...")
        print("Note: This requires Google Drive authentication")
        print("Skipping actual test to avoid modifying user's spreadsheets")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/gdrive/server.py
================================================
"""Google Drive MCP Server Configuration"""
from agents.mcp import MCPServerStdio
from typing import Optional
import os
import json

# Singleton server instance
_server: Optional[MCPServerStdio] = None
_connected: bool = False

def _ensure_oauth_keys():
    """Ensure OAuth keys file exists from environment variable"""
    creds_json = os.getenv('GDRIVE_CREDENTIALS_JSON')
    if not creds_json:
        return  # Keys might already exist or using individual env vars
    
    creds_dir = os.getenv('GDRIVE_CREDS_DIR', './mnt/mcp-creds')
    os.makedirs(creds_dir, exist_ok=True)
    creds_file = os.path.join(creds_dir, 'gcp-oauth.keys.json')
    
    # Only create if it doesn't exist
    if not os.path.exists(creds_file):
        try:
            creds_data = json.loads(creds_json)
            with open(creds_file, 'w') as f:
                json.dump(creds_data, f, indent=2)
        except Exception:
            pass  # Silently fail, let MCP handle the error

def get_server() -> MCPServerStdio:
    """Get the Google Drive MCP server instance (singleton)"""
    global _server
    if _server is None:
        # Ensure OAuth keys exist from environment variable
        _ensure_oauth_keys()
        
        _server = MCPServerStdio(
            name="GoogleDrive",
            params={
                "command": "npx",
                "args": ["-y", "@isaacphi/mcp-gdrive"],
                "env": {
                    "GDRIVE_CREDS_DIR": os.getenv("GDRIVE_CREDS_DIR", "./mnt/mcp-creds")
                }
            },
            cache_tools_list=True,
            client_session_timeout_seconds=60  # Increased for OAuth flow
        )
    return _server

async def ensure_connected() -> MCPServerStdio:
    """Ensure the server is connected, connect if not"""
    global _connected
    server = get_server()
    
    if not _connected:
        await server.connect()
        _connected = True
    
    return server

async def call_tool(tool_name: str, arguments: dict):
    """Call a tool on the MCP server with the given arguments"""
    server = await ensure_connected()
    
    # Get the session
    session = getattr(server, 'session', None) or getattr(server, '_client_session', None)
    if not session:
        raise RuntimeError("Could not access MCP session")
    
    # Call the tool
    result = await session.call_tool(name=tool_name, arguments=arguments)
    
    # Extract content from result
    if hasattr(result, 'content'):
        content = result.content
        if isinstance(content, list) and len(content) > 0:
            return content[0].text if hasattr(content[0], 'text') else str(content[0])
        return str(content)
    
    return result

# Tool discovery - run with: python ./servers/gdrive/server.py
if __name__ == "__main__":
    import asyncio
    
    async def discover_tools():
        print("Discovering tools from Google Drive MCP server...")
        server = await ensure_connected()
        
        session = getattr(server, 'session', None) or getattr(server, '_client_session', None)
        if session:
            tools_result = await session.list_tools()
            tools = tools_result.tools if hasattr(tools_result, 'tools') else []
            
            print(f"\nFound {len(tools)} tools:\n")
            for tool in tools:
                print(f"Tool: {tool.name}")
                print(f"  Description: {tool.description}")
                if hasattr(tool, 'inputSchema') and 'properties' in tool.inputSchema:
                    print(f"  Parameters:")
                    schema = tool.inputSchema
                    for param_name, param_info in schema['properties'].items():
                        param_type = param_info.get('type', 'any')
                        param_desc = param_info.get('description', '')
                        required = param_name in schema.get('required', [])
                        print(f"    - {param_name}: {param_type} {'(required)' if required else '(optional)'}")
                        if param_desc:
                            print(f"      {param_desc}")
                print()
    
    asyncio.run(discover_tools())




================================================
FILE: servers/notion/__init__.py
================================================
"""
Notion MCP Tools

Progressive disclosure pattern - import only what you need.
See: https://www.anthropic.com/engineering/code-execution-with-mcp
"""

# Server management
from .server import get_server, ensure_connected, call_tool

# Individual tools
from .notion_search import notion_search
from .notion_fetch import notion_fetch
from .notion_create_pages import notion_create_pages
from .notion_update_page import notion_update_page
from .notion_move_pages import notion_move_pages
from .notion_duplicate_page import notion_duplicate_page
from .notion_create_database import notion_create_database
from .notion_update_database import notion_update_database
from .notion_create_comment import notion_create_comment
from .notion_get_comments import notion_get_comments
from .notion_get_teams import notion_get_teams
from .notion_get_users import notion_get_users
from .notion_list_agents import notion_list_agents
from .notion_get_self import notion_get_self
from .notion_get_user import notion_get_user

__all__ = [
    # Server
    "get_server",
    "ensure_connected",
    "call_tool",
    # Tools
    "notion_search",
    "notion_fetch",
    "notion_create_pages",
    "notion_update_page",
    "notion_move_pages",
    "notion_duplicate_page",
    "notion_create_database",
    "notion_update_database",
    "notion_create_comment",
    "notion_get_comments",
    "notion_get_teams",
    "notion_get_users",
    "notion_list_agents",
    "notion_get_self",
    "notion_get_user",
]




================================================
FILE: servers/notion/notion_create_comment.py
================================================
from typing import Dict, Any, List
from .server import call_tool


async def notion_create_comment(
    parent: Dict[str, Any],
    rich_text: List[Any]
) -> str:
    """
    Add a comment to a page

    Args:
        parent: The parent of the comment. This must be a page.
        rich_text: An array of rich text objects that represent the content of the comment.

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {
        "parent": parent,
        "rich_text": rich_text
    }

    # Call the MCP tool
    return await call_tool("notion-create-comment", arguments)


# Test - run with: python ./servers/notion/notion_create_comment.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_create_comment...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid creating comments in user's Notion")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_create_database.py
================================================
from typing import Optional, Dict, Any, List
from .server import call_tool


async def notion_create_database(
    properties: Dict[str, Any],
    parent: Optional[Dict[str, Any]] = None,
    title: Optional[List[Any]] = None,
    description: Optional[List[Any]] = None
) -> str:
    """
    Creates a new Notion database with the specified properties schema.

    Args:
        properties: The property schema of the new database. If no title property is provided, one will be automatically added.
        parent: The parent under which to create the new database. If omitted, the database will be created as a private page at the workspace level.
        title: The title of the new database, as a rich text object.
        description: The description of the new database, as a rich text object.

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"properties": properties}

    # Add optional params only if provided
    if parent:
        arguments["parent"] = parent
    if title:
        arguments["title"] = title
    if description:
        arguments["description"] = description

    # Call the MCP tool
    return await call_tool("notion-create-database", arguments)


# Test - run with: python ./servers/notion/notion_create_database.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_create_database...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid creating test databases in user's Notion")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_create_pages.py
================================================
from typing import Optional, List, Dict, Any
from .server import call_tool


async def notion_create_pages(
    pages: List[Dict[str, Any]],
    parent: Optional[Dict[str, Any]] = None
) -> str:
    """
    Creates one or more Notion pages, with the specified properties and content.

    Args:
        pages: The pages to create.
        parent: The parent under which the new pages will be created. This can be a page (page_id), a database page (database_id), or a data source/collection under a database (data_source_id). If omitted, the new pages will be created as private pages at the workspace level. Use data_source_id when you have a collection:// URL from the fetch tool.

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"pages": pages}

    # Add optional params only if provided
    if parent:
        arguments["parent"] = parent

    # Call the MCP tool
    return await call_tool("notion-create-pages", arguments)


# Test - run with: python ./servers/notion/notion_create_pages.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_create_pages...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid creating test pages in user's Notion")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_duplicate_page.py
================================================
from .server import call_tool


async def notion_duplicate_page(page_id: str) -> str:
    """
    Duplicate a Notion page.

    Args:
        page_id: The ID of the page to duplicate. This is a v4 UUID, with or without dashes, and can be parsed from a Notion page URL.

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"page_id": page_id}

    # Call the MCP tool
    return await call_tool("notion-duplicate-page", arguments)


# Test - run with: python ./servers/notion/notion_duplicate_page.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_duplicate_page...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid duplicating user's Notion pages")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_fetch.py
================================================
from .server import call_tool


async def notion_fetch(id: str) -> str:
    """
    Retrieves details about a Notion entity (page or database) by URL or ID.

    Args:
        id: The ID or URL of the Notion page to fetch

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"id": id}

    # Call the MCP tool
    return await call_tool("notion-fetch", arguments)


# Test - run with: python ./servers/notion/notion_fetch.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_fetch...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid authentication requirements")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_get_comments.py
================================================
from .server import call_tool


async def notion_get_comments(page_id: str) -> str:
    """
    Get all comments of a page

    Args:
        page_id: Identifier for a Notion page.

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"page_id": page_id}

    # Call the MCP tool
    return await call_tool("notion-get-comments", arguments)


# Test - run with: python ./servers/notion/notion_get_comments.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_get_comments...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid authentication requirements")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_get_self.py
================================================
from .server import call_tool


async def notion_get_self() -> str:
    """
    Retrieve your token's bot user

    Returns:
        Tool result as string
    """
    # Build arguments dict
    arguments = {}

    # Call the MCP tool
    return await call_tool("notion-get-self", arguments)


# Test - run with: python ./servers/notion/notion_get_self.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_get_self...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid authentication requirements")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_get_teams.py
================================================
from typing import Optional
from .server import call_tool


async def notion_get_teams(query: Optional[str] = None) -> str:
    """
    Retrieves a list of teams (teamspaces) in the current workspace.

    Args:
        query: Optional search query to filter teams by name (case-insensitive).

    Returns:
        Tool result as string
    """
    # Build arguments dict
    arguments = {}

    # Add optional params only if provided
    if query:
        arguments["query"] = query

    # Call the MCP tool
    return await call_tool("notion-get-teams", arguments)


# Test - run with: python ./servers/notion/notion_get_teams.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_get_teams...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid authentication requirements")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_get_user.py
================================================
from typing import Dict, Any
from .server import call_tool


async def notion_get_user(path: Dict[str, Any]) -> str:
    """
    Retrieve a user

    Args:
        path: Path object

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"path": path}

    # Call the MCP tool
    return await call_tool("notion-get-user", arguments)


# Test - run with: python ./servers/notion/notion_get_user.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_get_user...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid authentication requirements")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_get_users.py
================================================
from typing import Optional
from .server import call_tool


async def notion_get_users(
    query: Optional[str] = None,
    start_cursor: Optional[str] = None,
    page_size: Optional[int] = None
) -> str:
    """
    Retrieves a list of users in the current workspace.

    Args:
        query: Optional search query to filter users by name or email (case-insensitive).
        start_cursor: Cursor for pagination. Use the next_cursor value from the previous response to get the next page.
        page_size: Number of users to return per page (default: 100, max: 100).

    Returns:
        Tool result as string
    """
    # Build arguments dict
    arguments = {}

    # Add optional params only if provided
    if query:
        arguments["query"] = query
    if start_cursor:
        arguments["start_cursor"] = start_cursor
    if page_size:
        arguments["page_size"] = page_size

    # Call the MCP tool
    return await call_tool("notion-get-users", arguments)


# Test - run with: python ./servers/notion/notion_get_users.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_get_users...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid authentication requirements")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_list_agents.py
================================================
from typing import Optional
from .server import call_tool


async def notion_list_agents(query: Optional[str] = None) -> str:
    """
    Retrieves a list of all custom agents (workflows) that the authenticated user has access to.

    Args:
        query: Optional search query to filter agents by name or description (case-insensitive).

    Returns:
        Tool result as string
    """
    # Build arguments dict
    arguments = {}

    # Add optional params only if provided
    if query:
        arguments["query"] = query

    # Call the MCP tool
    return await call_tool("notion-list-agents", arguments)


# Test - run with: python ./servers/notion/notion_list_agents.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_list_agents...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid authentication requirements")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_move_pages.py
================================================
from typing import List, Any
from .server import call_tool


async def notion_move_pages(
    page_or_database_ids: List[str],
    new_parent: Any
) -> str:
    """
    Move one or more Notion pages or databases to a new parent.

    Args:
        page_or_database_ids: An array of up to 100 page or database IDs to move. IDs are v4 UUIDs and can be supplied with or without dashes (e.g. extracted from a <page> or <database> URL given by the "search" or "fetch" tool). Data Sources under Databases can't be moved individually.
        new_parent: The new parent under which the pages will be moved. This can be a page, the workspace, a database, or a specific data source under a database when there are multiple. Moving pages to the workspace level adds them as private pages and should rarely be used.

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {
        "page_or_database_ids": page_or_database_ids,
        "new_parent": new_parent
    }

    # Call the MCP tool
    return await call_tool("notion-move-pages", arguments)


# Test - run with: python ./servers/notion/notion_move_pages.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_move_pages...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid moving user's Notion pages")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_search.py
================================================
from typing import Optional, Dict, Any
from .server import call_tool


async def notion_search(
    query: str,
    query_type: Optional[str] = None,
    data_source_url: Optional[str] = None,
    page_url: Optional[str] = None,
    teamspace_id: Optional[str] = None,
    filters: Optional[Dict[str, Any]] = None
) -> str:
    """
    Perform a search over Notion workspace and connected sources.

    Args:
        query: Semantic search query over your entire Notion workspace and connected sources (Slack, Google Drive, Github, Jira, Microsoft Teams, Sharepoint, OneDrive, or Linear). For best results, don't provide more than one question per tool call. Use a separate "search" tool call for each search you want to perform. Alternatively, the query can be a substring or keyword to find users by matching against their name or email address. For example: "john" or "john@example.com"
        query_type: Optional query type
        data_source_url: Optionally, provide the URL of a Data source to search. This will perform a semantic search over the pages in the Data Source. Note: must be a Data Source, not a Database. <data-source> tags are part of the Notion flavored Markdown format returned by tools like fetch. The full spec is available in the create-pages tool description.
        page_url: Optionally, provide the URL or ID of a page to search within. This will perform a semantic search over the content within and under the specified page. Accepts either a full page URL (e.g. https://notion.so/workspace/Page-Title-1234567890) or just the page ID (UUIDv4) with or without dashes.
        teamspace_id: Optionally, provide the ID of a teamspace to restrict search results to. This will perform a search over content within the specified teamspace only. Accepts the teamspace ID (UUIDv4) with or without dashes.
        filters: Optionally provide filters to apply to the search results. Only valid when query_type is 'internal'.

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"query": query}

    # Add optional params only if provided
    if query_type:
        arguments["query_type"] = query_type
    if data_source_url:
        arguments["data_source_url"] = data_source_url
    if page_url:
        arguments["page_url"] = page_url
    if teamspace_id:
        arguments["teamspace_id"] = teamspace_id
    if filters:
        arguments["filters"] = filters

    # Call the MCP tool
    return await call_tool("notion-search", arguments)


# Test - run with: python ./servers/notion/notion_search.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_search...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid authentication requirements")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_update_database.py
================================================
from typing import Optional, List, Dict, Any
from .server import call_tool


async def notion_update_database(
    database_id: str,
    title: Optional[List[Any]] = None,
    description: Optional[List[Any]] = None,
    properties: Optional[Dict[str, Any]] = None,
    is_inline: Optional[bool] = None,
    in_trash: Optional[bool] = None
) -> str:
    """
    Update a Notion database's properties, name, description, or other attributes.

    Args:
        database_id: The ID of the database to update. This is a UUID v4, with or without dashes, and can be parsed from a database URL.
        title: The new title of the database, as a rich text object, if you want to update it.
        description: The new description of the database, as a rich text object, if you want to update it.
        properties: Updates to make to the database's schema. Use null to remove a property, or provide the `name` only to rename a property.
        is_inline: Optional is_inline flag
        in_trash: Optional in_trash flag

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"database_id": database_id}

    # Add optional params only if provided
    if title:
        arguments["title"] = title
    if description:
        arguments["description"] = description
    if properties:
        arguments["properties"] = properties
    if is_inline is not None:
        arguments["is_inline"] = is_inline
    if in_trash is not None:
        arguments["in_trash"] = in_trash

    # Call the MCP tool
    return await call_tool("notion-update-database", arguments)


# Test - run with: python ./servers/notion/notion_update_database.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_update_database...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid modifying user's Notion databases")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/notion_update_page.py
================================================
from typing import Any
from .server import call_tool


async def notion_update_page(data: Any) -> str:
    """
    Update a Notion page's properties or content.

    Args:
        data: The data required for updating a page

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"data": data}

    # Call the MCP tool
    return await call_tool("notion-update-page", arguments)


# Test - run with: python ./servers/notion/notion_update_page.py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing notion_update_page...")
        print("Note: This requires Notion authentication via mcp-remote OAuth")
        print("Skipping actual test to avoid modifying user's Notion pages")
        print("✓ Tool file created successfully")

    asyncio.run(test())




================================================
FILE: servers/notion/server.py
================================================
"""Notion MCP Server Configuration"""
from agents.mcp import MCPServerStdio
from typing import Optional
import os

# Get the absolute path to the project root
folder_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))

# Singleton server instance
_server: Optional[MCPServerStdio] = None
_connected: bool = False

def get_server() -> MCPServerStdio:
    """Get the Notion MCP server instance (singleton)"""
    global _server
    if _server is None:
        _server = MCPServerStdio(
            name="Notion",
            params={
                "command": "npx",
                "args": ["-y", "mcp-remote", "https://mcp.notion.com/mcp"],
                "env": {
                    "MCP_REMOTE_CONFIG_DIR": os.path.join(folder_path, "mnt", "mcp-creds")
                }
            },
            cache_tools_list=True,
            client_session_timeout_seconds=30  # Increased for OAuth
        )
    return _server

async def ensure_connected() -> MCPServerStdio:
    """Ensure the server is connected, connect if not"""
    global _connected
    server = get_server()
    
    if not _connected:
        await server.connect()
        _connected = True
    
    return server

async def call_tool(tool_name: str, arguments: dict):
    """Call a tool on the MCP server with the given arguments"""
    server = await ensure_connected()
    
    # Get the session
    session = getattr(server, 'session', None) or getattr(server, '_client_session', None)
    if not session:
        raise RuntimeError("Could not access MCP session")
    
    # Call the tool
    result = await session.call_tool(name=tool_name, arguments=arguments)
    
    # Extract content from result
    if hasattr(result, 'content'):
        content = result.content
        if isinstance(content, list) and len(content) > 0:
            return content[0].text if hasattr(content[0], 'text') else str(content[0])
        return str(content)
    
    return result

# Tool discovery - run with: python ./servers/notion/server.py
if __name__ == "__main__":
    import asyncio
    
    async def discover_tools():
        print("Discovering tools from Notion MCP server...")
        server = await ensure_connected()
        
        session = getattr(server, 'session', None) or getattr(server, '_client_session', None)
        if session:
            tools_result = await session.list_tools()
            tools = tools_result.tools if hasattr(tools_result, 'tools') else []
            
            print(f"\nFound {len(tools)} tools:\n")
            for tool in tools:
                print(f"Tool: {tool.name}")
                print(f"  Description: {tool.description}")
                if hasattr(tool, 'inputSchema') and 'properties' in tool.inputSchema:
                    print(f"  Parameters:")
                    schema = tool.inputSchema
                    for param_name, param_info in schema['properties'].items():
                        param_type = param_info.get('type', 'any')
                        param_desc = param_info.get('description', '')
                        required = param_name in schema.get('required', [])
                        print(f"    - {param_name}: {param_type} {'(required)' if required else '(optional)'}")
                        if param_desc:
                            print(f"      {param_desc}")
                print()
    
    asyncio.run(discover_tools())




================================================
FILE: .claude/README.md
================================================
# Agency Swarm Claude Code Sub-Agents

Specialized agents for building production-ready Agency Swarm v1.0.0 multi-agent systems using a phased, test-driven workflow.

## Agents

- **api-researcher**: Researches MCP servers and APIs, saves docs locally, provides API key instructions
- **prd-creator**: Transforms concepts into PRDs with API docs, minimizes agent count (4-16 tools/agent)
- **agent-creator**: Creates agent modules and folders (no instructions)
- **instructions-writer**: Writes optimized instructions using prompt engineering best practices
- **tools-creator**: Implements and tests tools (MCP servers preferred, best practices, shared state)
- **qa-tester**: Wires agency, sends 5 test queries, suggests improvements, enables iteration

## Phased Execution Workflow

1. **Research**: api-researcher finds APIs/MCPs, documents how to get API keys
2. **Design**: prd-creator drafts PRD (strict agent count, tool mapping)
3. **Confirm**: User must approve PRD before proceeding
4. **API Keys**: Collect all required API keys with instructions
5. **Phase 1**: agent-creator and instructions-writer run in parallel
6. **Phase 2**: tools-creator runs after agent files exist, implements and tests all tools
7. **Test**: qa-tester sends 5 diverse queries, reports results, suggests improvements
8. **Iterate**: Claude orchestrator delegates fixes to tools-creator or instructions-writer until all tests pass

## Best Practices
- **MCP Integration**: Tools-creator adds MCP servers directly to agent files ([docs](https://agency-swarm.ai/core-framework/tools/mcp-integration))
- **Custom Tools**: Use chain-of-thought, type validation, error hints, test cases ([best practices](https://agency-swarm.ai/core-framework/tools/custom-tools/best-practices))
- **Shared State**: Use `self._shared_state` for tool data exchange ([docs](https://agency-swarm.ai/additional-features/shared-state))
- **Strict File Ownership**: Each agent only edits its own files
- **QA-Driven Iteration**: qa-tester drives improvements until agency is production-ready

## Usage

```
User: "Create a customer support agency"
→ Claude researches APIs and MCPs
→ Claude creates PRD and gets user approval
→ Claude collects all API keys
→ Claude runs agent-creator + instructions-writer (Phase 1)
→ Claude runs tools-creator (Phase 2)
→ Claude tests with 5 queries, iterates until all pass
→ Result: working agency/
```

See CLAUDE.md for complete orchestration details.


================================================
FILE: .claude/agents/agent-creator.md
================================================
---
name: agent-creator
description: Create complete agent modules with folder structure per Agency Swarm v1.0.0 spec
tools: Write, Read, Bash, MultiEdit
color: green
model: sonnet
---

Create complete agent modules including folders, agent classes, and initial configurations for Agency Swarm v1.0.0 agencies.

## Background

Agency Swarm v1.0.0 uses the OpenAI Agents SDK. Agents are instantiated directly (not subclassed). Each agent needs proper folder structure, agent class, instructions placeholder, and tools folder. All agencies require OpenAI API key.

## Input

- PRD path with agents, roles, and tool requirements
- Agency Swarm docs location: `ai_docs/agency-swarm/docs/`
- Communication flow pattern for the agency
- Note: Working in parallel with instructions-writer, BEFORE tools-creator

## Exact Folder Structure (v1.0.0)

```
├── example_agent/
│   ├── __init__.py
│   ├── agent_name.py       # Agent instantiation
│   ├── instructions.md     # Placeholder for instructions-writer
│   └── tools/              # For tools-creator to populate
├── example_agent2/
│   ├── __init__.py
│   ├── example_agent2.py
│   ├── instructions.md
│   └── tools/
├── agency.py               # Main agency file
├── agency_manifesto.md     # Shared instructions
├── requirements.txt        # Dependencies
└── .env                   # API keys template
```

## Agent Module Template (example_agent.py)

```python
from agents import ModelSettings
from agency_swarm import Agent

example_agent = Agent(
    name="AgentName",
    description="[Agent role from PRD]",
    instructions="./instructions.md",
    tools_folder="./tools",
    model_settings=ModelSettings(
        model="gpt-4o",
        temperature=0.5,
        max_completion_tokens=25000,
    ),
)
```

## Agent **init**.py Template

```python
sfrom .example_agent import example_agent

__all__ = ["example_agent"]
```

## Agency.py Template

```python
from dotenv import load_dotenv
from agency_swarm import Agency
# Agent imports will be added here

load_dotenv()

# Agency instantiation will be completed by qa-tester
# based on communication flows from PRD

if __name__ == "__main__":
    # This will be wired by qa-tester
    pass
```

## Agency Manifesto Template

```markdown
# Agency Manifesto

## Mission

[Agency mission from PRD]

## Working Principles

1. Clear communication between agents
2. Efficient task delegation
3. Quality output delivery
4. Continuous improvement through testing

## Standards

- All agents must validate inputs before processing
- Errors should be handled gracefully
- Communication should be concise and actionable
- Use MCP servers when available over custom tools
```

## Requirements.txt Template

```
agency-swarm>=1.0.0
python-dotenv
openai>=1.0.0
pydantic>=2.0.0
# Additional dependencies will be added by tools-creator
```

## .env Template

```
OPENAI_API_KEY=
# Additional API keys will be identified by tools-creator
```

## Process

1. Read PRD to extract:
   - Agency name (lowercase with underscores)
   - Agent names and descriptions
   - Communication pattern
2. Create main agency folder
3. For each agent in PRD:
   - Create agent folder with exact structure
   - Create example_agent.py with Agent instantiation (not subclass)
   - Use snake_case for instance names, PascalCase for Agent name parameter
   - Create **init**.py for imports
   - Create empty tools/ folder (tools-creator will populate)
   - **DO NOT create instructions.md** (instructions-writer owns this file)
4. Create agency-level files:
   - agency.py with import structure and communication flow pattern
   - agency_manifesto.md from template with PRD mission
   - requirements.txt with base dependencies
   - .env template with OPENAI_API_KEY placeholder
5. Use proper naming conventions:
   - Folders: lowercase with underscores
   - Agent instances: snake_case (e.g., `ceo`, `developer`)
   - Agent name parameter: PascalCase (e.g., `"CEO"`, `"Developer"`)

## File Ownership (CRITICAL)

**agent-creator owns**:

- All folders structure
- example_agent.py files
- **init**.py files
- agency.py (skeleton)
- agency_manifesto.md
- requirements.txt
- .env

**agent-creator MUST NOT touch**:

- instructions.md files (owned by instructions-writer)
- Any files in tools/ folders (owned by tools-creator)

## Coordination with Parallel Agents

- **instructions-writer**: Creates instructions.md files in parallel
- **tools-creator**: Runs AFTER us (needs agent files to exist)

## Return Summary

Report back:

- Agency created at: `agency_name/`
- Agent modules created: [list of example_agent.py files]
- Folder structure ready for tools and instructions
- Base requirements.txt created
- .env template ready for API keys



================================================
FILE: .claude/agents/api-researcher.md
================================================
---
name: api-researcher
description: Research MCP servers and APIs, prioritize MCP over custom implementations
tools: WebSearch, WebFetch, Write, Read
color: purple
model: sonnet
---

Research MCP servers and APIs for Agency Swarm v1.0.0 tool implementation, strongly prioritizing MCP servers.

## Background

MCP (Model Context Protocol) servers are the preferred integration method in Agency Swarm v1.0.0. They provide:

- Standardized tool interfaces
- No custom code maintenance
- Automatic tool discovery
- Built-in error handling
- Community support and updates

## Research Priority

1. **Built-in Tools First**: Built in tools are preferred over MCP servers.
2. **MCP Servers First**: Always check for existing MCP servers
3. **Official MCP Registry**: https://github.com/modelcontextprotocol/servers
4. **NPM Packages**: Search `@modelcontextprotocol/*`
5. **Community MCP**: Search GitHub for `mcp-server-*` repos
6. **Custom APIs Last**: Only if no MCP server exists

## Known MCP Servers

Common MCP servers to check for:

- `@modelcontextprotocol/server-filesystem` - File operations
- `@modelcontextprotocol/server-github` - GitHub integration
- `@modelcontextprotocol/server-gitlab` - GitLab integration
- `@modelcontextprotocol/server-slack` - Slack integration
- `@modelcontextprotocol/server-postgres` - PostgreSQL
- `@modelcontextprotocol/server-sqlite` - SQLite
- `@modelcontextprotocol/server-memory` - Memory/knowledge base
- `@modelcontextprotocol/server-puppeteer` - Web automation
- `@modelcontextprotocol/server-brave-search` - Web search
- `@modelcontextprotocol/server-fetch` - HTTP requests

## Web Search Tool (Built-in)

Agency Swarm has a built-in tool for web searchs. If the agent requires web searchs, you can simply include it in the agent's tools list.

```python
from agents.tool import WebSearchTool

tools = [WebSearchTool()]
```

## Process

1. Understand agency's functionality needs from concept
2. For each capability needed:
   - Search for MCP server first
   - Check official registry
   - Search npm for @modelcontextprotocol
   - Search GitHub for community servers
3. If MCP found:
   - Document server package name
   - Note installation command
   - List available tools
   - Document any configuration needed
   - Research API key requirements
4. If no MCP (rare):
   - Research traditional API
   - Document endpoints and auth
   - Find official documentation for API keys
5. **Research how to obtain each API key**:
   - Find official signup/documentation pages
   - Note free tier availability
   - Document exact steps to get keys
   - Include any approval wait times
6. Save findings to `agency_name/api_docs.md`

# Output Format

Create `agency_name/api_docs.md`:

```markdown
# API Documentation for [Agency Name]

## MCP Servers Available

### File Operations

- **Package**: `@modelcontextprotocol/server-filesystem`
- **Installation**: `npx -y @modelcontextprotocol/server-filesystem .`
- **Tools Provided**:
  - read_file: Read file contents
  - write_file: Create or update files
  - list_directory: List directory contents
  - create_directory: Create new directories
  - delete_file: Delete files
  - move_file: Move or rename files
- **Configuration**: Working directory path as argument
- **API Keys**: None required

### GitHub Integration

- **Package**: `@modelcontextprotocol/server-github`
- **Installation**: `npx -y @modelcontextprotocol/server-github`
- **Tools Provided**:
  - create_issue: Create GitHub issues
  - create_pull_request: Create PRs
  - list_issues: Get repository issues
  - push_files: Push files to repository
- **Configuration**: Repository name
- **API Keys**: GITHUB_TOKEN required
- **How to get GITHUB_TOKEN**:
  1. Go to https://github.com/settings/tokens
  2. Click "Generate new token (classic)"
  3. Name it and select scopes: repo, workflow
  4. Copy the token immediately (won't be shown again)

## Traditional APIs (Only if no MCP)

### [API Name]

- **Base URL**: https://api.example.com
- **Authentication**: Bearer token
- **Key Endpoints**:
  - GET /resource - List resources
  - POST /resource - Create resource
- **Rate Limits**: 100 requests/hour
- **API Keys**: API_KEY required
- **How to get API_KEY**:
  1. Visit [official website]
  2. Sign up for account
  3. Navigate to API section
  4. Generate new API key
  5. Note any approval wait time

## API Key Instructions

### OPENAI_API_KEY (Required for all agencies)

**How to obtain**:

1. Go to https://platform.openai.com/api-keys
2. Sign up or log in to OpenAI account
3. Click "Create new secret key"
4. Name your key (e.g., "agency-swarm")
5. Copy and save the key immediately
6. Add billing details at https://platform.openai.com/account/billing
7. Minimum $5 credit recommended for testing

### [OTHER_API_KEY]

**How to obtain**:
[Specific steps for this API]
**Free tier**: [Yes/No, limitations]
**Approval time**: [Immediate/X days]

## Summary

- MCP servers found: [count]
- Traditional APIs needed: [count]
- Total API keys required:
  - OPENAI_API_KEY (always) - $5 minimum
  - [List other keys with cost notes]
```

## MCP Server Benefits to Emphasize

When MCP servers are available, note these advantages:

- Zero maintenance of tool code
- Automatic updates from community
- Standardized error handling
- Tool discovery built-in
- Reduced agency complexity

## Return Summary

Report back:

- File saved at: `agency_name/api_docs.md`
- MCP servers found: [count and names]
- Coverage: [X]% of needs covered by MCP
- Custom APIs required: [list if any]
- API keys needed: [complete list]
- Recommendation: Use MCP servers for [specific functions]



================================================
FILE: .claude/agents/instructions-writer.md
================================================
---
name: instructions-writer
description: Write optimized agent instructions using prompt engineering best practices
tools: Write, Read, MultiEdit
color: yellow
model: sonnet
---

Write and refine Agency Swarm v1.0.0 agent instructions using prompt engineering best practices for maximum clarity and performance.

## Background
Agency Swarm agents need clear, actionable instructions that follow prompt engineering best practices. Instructions must be specific, example-driven, and integrate tools directly into numbered steps. Working in parallel with agent-creator and tools-creator during initial creation.

## Prompt Engineering Principles
Based on best practices:
1. **Start Simple**: Use concise, verb-driven instructions
2. **Be Specific**: Explicitly state desired outputs and formats
3. **Provide Examples**: Include concrete examples of expected behavior
4. **Use Positive Instructions**: "Do this" rather than "Don't do that"
5. **Integrate Tools in Steps**: Show exactly when and how to use each tool
6. **Use Variables**: Parameterize dynamic values with placeholders
7. **Test Continuously**: Refine based on actual test results

## Input Modes

### Creation Mode (Parallel Execution)
- PRD path with agent roles, tasks, and workflows
- Communication flow pattern for the agency
- Agency Swarm docs reference: https://agency-swarm.ai
- Note: agent-creator creates folders in parallel, tools-creator runs AFTER us

### Refinement Mode (After Testing)
- Test results file path: `agency_name/test_results.md`
- Specific failures to address
- Performance metrics to improve

## Instructions Template (v1.0.0)
```markdown
# Role
You are **[specific role from PRD, e.g., "a data analysis expert specializing in financial reports"]**

# Task
Your task is to **[primary objective clearly stated]**:
- [Specific subtask 1]
- [Specific subtask 2]
- [Quality expectations]

# Context
- You are part of [agency name] agency
- You work alongside: [other agents and their roles]
- Your outputs will be used for: [downstream purpose]
- Key constraints: [time, format, or resource limitations]

# Examples

## Example 1: [Common Scenario Name]
**Input**: "[Sample user request or message from another agent]"
**Process**:
1. Parse the request for [specific elements]
2. Use ToolName to [specific action]
3. Validate results contain [required fields]
**Output**: "[Expected response format and content]"

## Example 2: [Edge Case Scenario]
**Input**: "[Unusual or error case]"
**Process**:
1. Detect [issue indicator]
2. Use ErrorHandlingTool to [recovery action]
3. Notify CEO agent with: "[specific message format]"
**Output**: "[Graceful error response]"

# Instructions
1. **Receive Request**: Parse incoming messages for [specific keywords/patterns]
2. **Validate Input**: Check that request contains [required fields] using format: `{field1: type, field2: type}`
3. **Gather Information**: Use [ToolName1] to retrieve [data type] when [condition]
4. **Process Data**: 
   - If [condition A]: Use [ToolName2] with parameters `{param1: value}`
   - If [condition B]: Use [ToolName3] to [specific action]
5. **Quality Check**: Verify output meets these criteria:
   - [Criterion 1 with measurable threshold]
   - [Criterion 2 with specific format]
6. **Format Response**: Structure output as:
   ```json
   {
     "status": "success/error",
     "data": {...},
     "next_steps": [...]
   }
   ```
7. **Send Results**: Use SendMessage to deliver to [target agent] with message type "[category]"
8. **Handle Errors**: 
   - On tool failure: Retry up to 3 times with exponential backoff
   - On invalid input: Return structured error with guidance
   - On timeout: Escalate to CEO with partial results

# Additional Notes
- Response time target: Under [X] seconds
- Use [MCP_Server.tool_name] for file operations (more reliable than custom tools)
- Always include confidence scores when making predictions
- Preserve message thread context for multi-turn conversations
- Log important decisions for audit trail
```

## MCP Server Tool Integration
When MCP servers are used, integrate them directly into steps:
```markdown
3. **Read Configuration**: Use `Filesystem_Server.read_file` to load settings from `config.json`
4. **Update Status**: Use `GitHub_Server.create_issue` with title format: "[STATUS] Task-{id}"
```

## Creation Process (Parallel Execution)

1. **Analyze PRD** for each agent:
   - Extract role with specific expertise
   - Identify primary tasks and subtasks
   - Note tool assignments from tools-creator
   - Understand position in communication flow

2. **Write Role Section**:
   - Be specific about expertise area
   - Use active voice and strong verbs
   - Include domain context

3. **Define Clear Task**:
   - Start with primary objective
   - Break down into measurable subtasks
   - Include quality expectations

4. **Provide Rich Context**:
   - Agency purpose and structure
   - Inter-agent relationships
   - Downstream dependencies
   - Operating constraints

5. **Create Concrete Examples**:
   - Common successful scenario
   - Error/edge case handling
   - Use actual tool names and parameters
   - Show exact input/output formats

6. **Write Numbered Instructions**:
   - Each step should be actionable
   - Integrate tools with specific conditions
   - Include decision branches
   - Specify exact formats and thresholds

7. **Add Operational Notes**:
   - Performance targets
   - Preferred tool choices
   - Common pitfalls to avoid
   - Escalation procedures

## Refinement Process (Test-Driven)

1. **Parse Test Results** for patterns:
   - Tool usage errors → Add specific parameters in steps
   - Format errors → Provide exact schemas
   - Logic errors → Add decision criteria
   - Performance issues → Optimize step order

2. **Update Specific Sections**:
   - Add examples for failed scenarios
   - Clarify ambiguous instructions
   - Add validation steps
   - Include error recovery procedures

3. **Maintain Simplicity**:
   - Keep language concise
   - Remove redundant instructions
   - Focus on observed issues only

## Quality Checklist
- [ ] Role is specific and expertise-focused
- [ ] Task has measurable objectives
- [ ] Context explains agency dynamics
- [ ] At least 2 concrete examples provided
- [ ] Tools integrated into numbered steps
- [ ] Error handling explicitly defined
- [ ] Output formats clearly specified
- [ ] Performance targets included
- [ ] Positive instructions used ("Do" not "Don't")
- [ ] Variables parameterized with placeholders

## File Ownership (CRITICAL)
**instructions-writer owns**:
- ALL instructions.md files in agent folders

**instructions-writer MUST NOT touch**:
- agent_name.py files (owned by agent-creator)
- __init__.py files (owned by agent-creator)
- Any files in tools/ folders (owned by tools-creator)
- agency.py (owned by agent-creator/qa-tester)

## Coordination with Parallel Agents
- **agent-creator**: Creates folder structure (we create instructions.md)
- **tools-creator**: Runs AFTER us (needs our instructions to test)
- We CREATE instructions.md files (not update existing ones)

## Return Summary
Report back:
- Instructions created for: [agent names with paths]
- Examples provided: [count per agent]
- Tools integrated into: [X] numbered steps
- Error handling steps: [count]
- Performance targets set: Yes/No
- Ready for testing with qa-tester


================================================
FILE: .claude/agents/prd-creator.md
================================================
---
name: prd-creator
description: Transform ideas into comprehensive PRDs optimized for parallel agent creation
tools: Write, Read
color: blue
model: sonnet
---

Create Product Requirements Documents for Agency Swarm v1.0.0 agencies, optimized for parallel agent creation.

## Background
Agency Swarm v1.0.0 is built on OpenAI's Agents SDK. Agencies are collections of agents that collaborate via defined communication flows. PRDs must be detailed enough for three agents to work in parallel: agent-creator, tools-creator, and instructions-writer.

## Input
- User concept/idea with clarified goals
- API/MCP documentation path: `agency_name/api_docs.md`
- Framework version: Agency Swarm v1.0.0
- Preferred communication pattern

## Key Design Principles
1. **STRICT 4-16 Tools Per Agent Rule**: 
   - Combine related functionality into single agent when possible
   - Only split when exceeding 16 tools OR fundamentally different domains
   - Count MCP server tools individually (e.g., filesystem server = 6 tools)
2. **Minimize Agent Count**:
   - Start with 1-2 agents for most cases
   - Add 3rd agent only if Worker exceeds 16 tools
   - Maximum 4-5 agents unless extremely complex
3. **Single Entry Point**: One agent (usually CEO) interfaces with user
4. **Clear Responsibilities**: No overlap between agents
5. **MCP First**: Prioritize MCP servers over custom tools
6. **Parallel-Ready**: Provide enough detail for simultaneous creation

## Communication Flow Patterns

### 1. Orchestrator-Workers (80% of cases)
Best for: Task delegation, report generation, multi-step processes
```
CEO → Worker1 (data gathering)
CEO → Worker2 (processing)
CEO → Worker3 (reporting)
```

### 2. Sequential Pipeline (15% of cases)
Best for: ETL, document processing, staged workflows
```
Collector → Processor → Publisher
(with SendMessageHandoff for automatic handoffs)
```

### 3. Collaborative Network (5% of cases)
Best for: Complex interdependent tasks, creative work
```
CEO ↔ Developer
CEO ↔ Designer
Developer ↔ Designer
```

## PRD Template
```markdown
# [Agency Name] - Product Requirements Document

## Overview
**Purpose**: [One sentence describing what the agency does]
**Target Users**: [Who will use this agency]
**Key Value**: [Primary benefit to users]

## Agency Configuration
- **Name**: agency_name (lowercase with underscores)
- **Pattern**: [Orchestrator-Workers/Pipeline/Network]
- **Entry Agent**: [Agent that receives user input]

## Agents

### Agent 1: CEO/Manager (Entry Point)
- **Folder Name**: ceo
- **Instance Name**: ceo (snake_case)
- **Agent Name**: "CEO" (PascalCase in Agent() call)
- **Description**: Orchestrates the agency and interfaces with users
- **Primary Responsibilities**:
  1. Accept and parse user requests
  2. Delegate tasks to specialized agents
  3. Synthesize results for user
  4. Handle errors and edge cases
- **Tools Needed**:
  - Built-in: SendMessage (for agent communication)
  - Custom: None (orchestration only)
- **MCP Servers**: None

### Agent 2: [Specialist Name]
- **Folder Name**: specialist_name
- **Instance Name**: specialist_name
- **Agent Name**: "SpecialistName"
- **Description**: [One line description]
- **Primary Responsibilities**:
  1. [Specific task 1]
  2. [Specific task 2]
  3. [Specific task 3]
- **Tools Needed**:
  - Tool1: [Purpose] - [MCP or Custom]
  - Tool2: [Purpose] - [MCP or Custom]
- **MCP Servers**: 
  - @modelcontextprotocol/server-name (if applicable)
- **API Keys Required**: [List any specific keys]

[Repeat for each agent...]

## Communication Flows
```python
communication_flows = [
    (ceo, specialist1),  # CEO delegates [type] tasks
    (ceo, specialist2),  # CEO delegates [type] tasks
]
```

## Tool Specifications

### MCP Server Tools (Preferred)
| Tool | Agent | MCP Server | Purpose |
|------|-------|------------|---------|
| filesystem ops | agent1 | @modelcontextprotocol/server-filesystem | File management |
| github ops | agent2 | @modelcontextprotocol/server-github | Repository interaction |

### Custom Tools (Only if no MCP)
| Tool | Agent | Type | Inputs | Output |
|------|-------|------|--------|--------|
| ToolName | agent1 | BaseTool | param1: str | Result string |

## Workflow Examples

### Example 1: [Common Use Case]
**User Input**: "[Sample user request]"
**Flow**:
1. CEO receives request
2. CEO analyzes and delegates to Agent2: "[specific task]"
3. Agent2 uses Tool1 to [action]
4. Agent2 returns result to CEO
5. CEO formats and returns to user

### Example 2: [Error Case]
**Scenario**: [What could go wrong]
**Handling**: [How agency handles it]

## Dependencies
- **Required API Keys**:
  - OPENAI_API_KEY (always required)
  - [Additional keys from tools]
- **Python Packages**:
  - agency-swarm>=1.0.0
  - python-dotenv
  - [Tool-specific packages]

## Success Metrics
- [ ] All agents respond to their designated tasks
- [ ] Communication flows work bidirectionally
- [ ] Error messages are clear and actionable
- [ ] Response time under [X] seconds
- [ ] MCP servers initialize correctly

## Parallel Creation Notes
This PRD is designed for parallel execution:
- **agent-creator**: Use agent specifications to create modules
- **tools-creator**: Use tool specifications and MCP servers
- **instructions-writer**: Use responsibilities and workflows
```

## Process
1. Read API docs to identify available MCP servers and count tools
2. **Apply Minimum Agent Strategy**:
   - Count total tools needed (MCP + custom)
   - If ≤8 tools: Use 1 agent (Worker)
   - If ≤16 tools: Use 2 agents (CEO + Worker)
   - If 17-32 tools: Use 3 agents (CEO + 2 specialists)
   - If 33-48 tools: Use 4 agents (CEO + 3 specialists)
   - Only exceed 4 agents for truly complex cases
3. **Group tools by function**:
   - Data collection tools → one agent
   - Processing/analysis tools → one agent
   - Reporting/output tools → can be in same agent
   - Don't split just for organization - only for tool count
4. Map tools to agents:
   - Pack agents to 10-14 tools each (room for growth)
   - Each tool belongs to ONE agent
   - Prefer MCP servers (count their tools)
5. Define minimal communication flows:
   - Use Orchestrator-Workers pattern (simplest)
   - Avoid complex networks unless essential
6. Create detailed workflow examples
7. Document all requirements
8. **Final check**: Can we reduce agent count further?
9. Save to `agency_name/prd.txt` with agent count justification

## Quality Checklist
- [ ] **Minimum agents used** (1-2 for most cases, 3-4 max)
- [ ] Each agent has 4-16 tools (aim for 10-14)
- [ ] Agent count justified by tool count, not organization
- [ ] No tool duplication across agents
- [ ] Communication flows are simple (prefer Orchestrator-Workers)
- [ ] MCP servers prioritized over custom tools
- [ ] Tool counts include all MCP server tools
- [ ] Workflow examples are concrete
- [ ] All API keys documented
- [ ] Could NOT reduce agent count further

## Return Summary
Report back:
- PRD created at: `agency_name/prd.txt`
- Agents defined: [count and names]
- Communication pattern: [which pattern]
- MCP coverage: [X]% of tools via MCP
- Custom tools needed: [count]
- API keys required: [complete list]
- Ready for parallel creation: Yes


================================================
FILE: .claude/agents/qa-tester.md
================================================
---
name: qa-tester
description: Wire agency and test with 5 example queries, provide improvement suggestions
tools: Write, Read, Bash, Edit, MultiEdit
color: red
model: sonnet
---

Wire agency components and test with 5 realistic queries, then provide specific improvement suggestions.

## Background
Agency Swarm v1.0.0 testing focuses on real-world usage. Tools are already tested by tools-creator. Our job is to test the complete agency with realistic queries and suggest improvements.

## Prerequisites
- API keys already collected and in .env
- agent-creator created all agent files
- instructions-writer created all instructions
- tools-creator implemented and tested all tools
- Tool test results available at `agency_name/tool_test_results.md`

## Testing Process

### 1. Wire agency.py
Complete the agency setup based on PRD:
```python
from dotenv import load_dotenv
from agency_swarm import Agency
from agent1_folder.agent1 import agent1
from agent2_folder.agent2 import agent2

load_dotenv()

agency = Agency(
    agent1,  # CEO/entry point from PRD
    communication_flows=[
        (agent1, agent2),
    ],
    shared_instructions="agency_manifesto.md",
)

if __name__ == "__main__":
    # Test with programmatic interface
    response = agency.get_completion("test query")
    print(response)
```

### 2. Quick Validation
```bash
# Verify all dependencies installed
pip list | grep agency-swarm

# Check tool test results
cat agency_name/tool_test_results.md
```

### 3. Generate 5 Test Queries
Based on PRD functionality, create 5 diverse test queries:
1. **Basic capability test** - Simple task using core functionality
2. **Multi-step workflow** - Task requiring agent collaboration
3. **Edge case handling** - Unusual but valid request
4. **Error recovery** - Invalid input or missing data
5. **Complex real-world scenario** - Comprehensive task

### 4. Execute Test Queries
Run each query and document:
```python
test_queries = [
    "Query 1: [Basic task from PRD]",
    "Query 2: [Multi-agent collaboration task]",
    "Query 3: [Edge case scenario]",
    "Query 4: [Error handling test]",
    "Query 5: [Complex real-world request]"
]

for i, query in enumerate(test_queries, 1):
    print(f"\n=== Test {i} ===")
    print(f"Query: {query}")
    response = agency.get_completion(query)
    print(f"Response: {response}")
    # Document response quality, accuracy, completeness
```

### 5. Create Comprehensive Test Report
Save to `agency_name/qa_test_results.md`:
```markdown
# QA Test Results - [timestamp]

## Agency Configuration
- Agents: [count and names]
- Communication pattern: [type]
- Tools per agent: [breakdown]

## Test Query Results

### Test 1: Basic Capability
**Query**: "[exact query]"
**Expected**: [what should happen based on PRD]
**Actual Response**: "[full response]"
**Quality Score**: 8/10
**Issues**:
- [Any problems observed]
**Status**: ✅ PASSED / ⚠️ PARTIAL / ❌ FAILED

### Test 2: Multi-Agent Collaboration
[Same format...]

### Test 3: Edge Case
[Same format...]

### Test 4: Error Handling
[Same format...]

### Test 5: Complex Scenario
[Same format...]

## Performance Metrics
- Average response time: [X] seconds
- Success rate: [X]/5 queries
- Error handling: [Good/Needs work]
- Response quality: [1-10 scale]
- Completeness: [1-10 scale]

## Improvement Suggestions

### For Instructions (instructions-writer)
1. **Agent: [name]** - Instruction unclear on [specific step]
   - Current: "[problematic instruction]"
   - Suggested: "[improved instruction]"
2. [Additional specific improvements]

### For Tools (tools-creator)
1. **Tool: [name]** - Needs better error handling
   - Issue: [specific problem]
   - Fix: [specific solution]
2. [Additional tool improvements]

### For Communication Flow
1. Consider adding [specific flow] for [reason]
2. [Other architectural suggestions]

## Overall Assessment
- **Ready for Production**: Yes/No
- **Critical Issues**: [list if any]
- **Recommended Next Steps**:
  1. [Specific action]
  2. [Specific action]
  3. [Specific action]

## Specific Files to Update
- `agent_name/instructions.md` - Lines X-Y need clarity
- `agent_name/tools/ToolName.py` - Add validation for [input]
- `agency.py` - Consider adding [feature]
```

### 6. Test Different Query Styles
Vary the query formats to test robustness:
- Direct commands: "Do X"
- Questions: "How can I...?"
- Complex requests: "I need to X, then Y, considering Z"
- Incomplete info: "Help me with [vague request]"
- Follow-ups: Test multi-turn conversations

## Key Testing Focus
1. **Realistic queries** - Use examples that real users would ask
2. **Actual task completion** - Verify the agency produces useful results
3. **Tool integration** - Ensure MCP servers and tools work correctly
4. **Error handling** - Test graceful failure modes
5. **Response quality** - Check for completeness and accuracy

## Return Summary
Report back:
- Test results saved at: `agency_name/qa_test_results.md`
- Tests passed: [X]/5
- Agency status: ✅ READY / ⚠️ NEEDS IMPROVEMENTS / ❌ MAJOR ISSUES
- Top 3 improvements needed:
  1. [Most important fix]
  2. [Second priority]
  3. [Third priority]
- Specific agents needing updates: [list with reasons]


================================================
FILE: .claude/agents/tools-creator.md
================================================
---
name: tools-creator
description: Implement and test tools with MCP servers preferred, runs after agent files exist
tools: Write, Read, Grep, MultiEdit, Bash
color: orange
model: sonnet
---

Implement production-ready Agency Swarm v1.0.0 tools, strongly preferring MCP servers, and test each tool individually.

## Background

Agency Swarm v1.0.0 strongly prefers MCP (Model Context Protocol) servers. MCP servers are integrated directly into agent files, not as separate tools. Runs AFTER agent-creator and instructions-writer complete.

## Input

- PRD path with tool requirements
- API docs path: `agency_name/api_docs.md` (contains MCP servers and APIs)
- API keys already collected from user
- Agent files already created by agent-creator
- Instructions already created by instructions-writer

## MCP Server Integration (CRITICAL - Based on Official Docs)

### Step 1: Identify MCP Servers from api_docs.md

Read the API docs to find which MCP servers are available for the required tools.

### Step 2: Update Agent Files with MCP Servers

For each agent that needs MCP tools, MODIFY the agent's .py file:

```python
from agency_swarm import Agent
from agency_swarm.tools.mcp import MCPServerStdio

# Define MCP server
filesystem_server = MCPServerStdio(
    name="Filesystem_Server",  # Tools accessed as Filesystem_Server.read_file
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", "."],
    },
    cache_tools_list=True
)

# Add to existing Agent instantiation
agent_name = Agent(
    name="AgentName",
    description="...",
    instructions="./instructions.md",
    tools_folder="./tools",
    mcp_servers=[filesystem_server],  # ADD THIS LINE
    model_settings=ModelSettings(
        model="gpt-4o",
        temperature=0.5,
        max_completion_tokens=25000,
    ),
)
```

### Common MCP Servers

```python
# GitHub Server
github_server = MCPServerStdio(
    name="GitHub_Server",
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-github"],
        "env": {"GITHUB_TOKEN": os.getenv("GITHUB_TOKEN")},
    },
    cache_tools_list=True
)

# Slack Server (if available)
slack_server = MCPServerStdio(
    name="Slack_Server",
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-slack"],
        "env": {"SLACK_TOKEN": os.getenv("SLACK_TOKEN")},
    },
    cache_tools_list=True
)
```

### Web Search Tool (Built-in)

You can use the following built-in tool when agent requires web searchs:

```python
from agents.tool import WebSearchTool

tools = [WebSearchTool()]
```

### Custom Tool Pattern (ONLY if no MCP)

Place in `agency_name/agent_name/tools/ToolName.py`:

```python
from agency_swarm.tools import BaseTool
from pydantic import Field
import os
from dotenv import load_dotenv

load_dotenv()

class ToolName(BaseTool):
    """Clear description for agent."""

    input_field: str = Field(..., description="Input description")

    def run(self):
        api_key = os.getenv("API_KEY_NAME")
        if not api_key:
            return "Error: API_KEY_NAME not found"

        try:
            # Real implementation
            result = self.perform_operation()
            return str(result)
        except Exception as e:
            return f"Error: {str(e)}"

if __name__ == "__main__":
    # Test with real data
    tool = ToolName(input_field="test_value")
    print(tool.run())
```

## Best Practices for Custom Tools

### 1. Chain-of-Thought for Complex Tools

For tools requiring multi-step planning:

```python
class ComplexAnalysisTool(BaseTool):
    """Performs complex analysis after planning the approach."""

    chain_of_thought: str = Field(
        ...,
        description="Think step-by-step about how to perform the analysis."
    )
    data: str = Field(..., description="Data to analyze.")

    def run(self):
        # The agent will fill chain_of_thought with its reasoning
        # Use this for logging or conditional logic
        return "Analysis complete."
```

### 2. Provide Next-Step Hints

Guide the agent on what to do next:

```python
class QueryDatabase(BaseTool):
    question: str = Field(...)

    def run(self):
        context = self.query_database(self.question)

        if context is None:
            # Tell agent what to do next
            raise ValueError("No context found. Please try a different search term or ask the user for clarification.")
        else:
            return context
```

### 3. Use Specific Types

Restrict inputs to valid values:

```python
from typing import Literal
from pydantic import EmailStr

class RunCommand(BaseTool):
    """Execute predefined system commands."""

    command: Literal["start", "stop", "restart"] = Field(
        ...,
        description="Command to execute: 'start', 'stop', or 'restart'."
    )

class EmailSender(BaseTool):
    recipient: EmailStr = Field(..., description="Valid email address.")
```

### 4. Use Shared State for Flow Control

Shared state is a centralized dictionary accessible by all tools and agents. Use it to share data without parameter passing.

#### Shared State Key Concepts

- **Shared State**: All tools within an agency share a Python dictionary (`self._shared_state`)
- **Data Sharing**: Tools can exchange data without explicit parameter passing
- **Flow Control**: Use shared state to enforce tool execution order
- **Note**: Shared state only works when tools are deployed with agents (not as separate APIs)

#### Common Shared State Patterns

1. **Data Collection → Processing**: Tool A collects data, Tool B processes it
2. **Multi-Step Workflows**: Each tool marks its completion for the next
3. **Session Management**: Store user session data across tools
4. **Cache Results**: Avoid redundant API calls by caching in shared state
5. **Error Context**: Store error details for debugging across tools

**Setting values**:

```python
class QueryDatabase(BaseTool):
    """Retrieves data and stores it in shared state."""
    question: str = Field(..., description="The query to execute.")

    def run(self):
        # Fetch data
        context = query_database(self.question)

        # Store in shared state for other tools to use
        self._shared_state.set('context', context)
        self._shared_state.set('query_timestamp', datetime.now())

        return "Context retrieved and stored successfully."
```

**Getting values**:

```python
class GenerateReport(BaseTool):
    """Generates report using data from shared state."""
    format: str = Field(..., description="Report format")

    def run(self):
        # Get data from shared state
        context = self._shared_state.get('context')
        timestamp = self._shared_state.get('query_timestamp')

        if not context:
            raise ValueError("No context found. Please run QueryDatabase first.")

        # Use the shared data
        report = self.generate_report(context, timestamp, self.format)
        return report
```

**Flow validation**:

```python
class Action2(BaseTool):
    input: str = Field(...)

    def run(self):
        # Check if previous action completed
        if self._shared_state.get("action_1_complete") != True:
            raise ValueError("Please complete Action1 first before proceeding.")

        # Perform action
        result = self.perform_action(self.input)

        # Mark this action as complete
        self._shared_state.set("action_2_complete", True)
        self._shared_state.set("action_2_result", result)

        return "Action 2 completed successfully."
```

### 5. Combine Multiple Methods

Make complex tools readable:

```python
class DataProcessor(BaseTool):
    """Process data through multiple stages."""

    input_data: str = Field(...)

    def run(self):
        # Step 1: Validate
        validated_data = self.validate_input(self.input_data)
        # Step 2: Process
        processed_data = self.process_data(validated_data)
        # Step 3: Format
        output = self.format_output(processed_data)
        return output

    def validate_input(self, data):
        # Validation logic
        return data

    def process_data(self, data):
        # Processing logic
        return data

    def format_output(self, data):
        # Formatting logic
        return data
```

## Process (Runs AFTER agent-creator and instructions-writer)

1. **Read PRD and API docs**:

   - Identify which agent needs which tools
   - Check which MCP servers are available

2. **For each agent's tools**:

   - **First check**: Is there an MCP server?
   - If YES → Update agent's .py file to add MCP server
   - If NO → Create custom tool in tools/ folder

3. **Implement MCP Servers** (CRITICAL):

   - Open the agent's .py file
   - Import MCPServerStdio at the top
   - Define the MCP server instance
   - Add `mcp_servers=[server_instance]` to Agent()
   - Test that the server initializes

4. **Test EVERY tool individually**:

   ```bash
   # Test custom tools
   python agency_name/agent_name/tools/ToolName.py

   # Test MCP server initialization
   python -c "from agency_name.agent_name import agent_name; print('MCP loaded')"
   ```

5. **Update and install requirements.txt**:

   - Add all new dependencies to requirements.txt
   - Make sure venv is activated
   - Run `pip install -r requirements.txt`

6. **Test and iterate on each tool**:
   Test each tool by running each ToolName.py file
   - Make sure each tool is working as expected
   - Apply best practices:
     - Add chain-of-thought for complex tools
     - Provide helpful error messages
     - Use specific types (Literal, EmailStr)
     - Implement shared state for data passing between tools
     - Add flow validation using shared state
     - Include proper test cases
   - If not working, fix the issue
   - Keep iterating until all tools pass tests
   - **Important**: Do not come back to the user until all tools are working as expected.

## File Ownership (CRITICAL)

**tools-creator owns**:

- All files in tools/ folders
- Modifications to agent .py files (ONLY for MCP servers)
- tool_test_results.md

**tools-creator MUST NOT touch**:

- instructions.md files
- **init**.py files
- agency.py (except imports if needed)

## Common Mistakes to Avoid

1. **DON'T create mcp_config.py** - MCP servers go directly in agent files
2. **DON'T skip testing** - Test every single tool with real data
3. **DON'T create custom tools if MCP exists**
4. **DO import os and load_dotenv for API keys**
5. **DO add all MCP servers to the agent's mcp_servers list**
6. **DON'T use print() in tools** - Return strings instead
7. **DO handle errors gracefully** - Return error messages, don't crash
8. **DO include test cases** - Every tool file needs if **name** == "**main**"
9. **DON'T hardcode values** - Use environment variables or parameters
10. **DO validate inputs** - Check types and ranges before processing

## Return Summary

Report back:

- MCP servers integrated into agents: [list with agent names]
- Custom tools created: [list with file paths]
- Test results saved at: `agency_name/tool_test_results.md`
- All tools tested: ✅/❌
- Failed tools needing fixes: [list]
- Best practices applied:
  - Chain-of-thought tools: [count]
  - Tools with type restrictions: [count]
  - Tools with helpful error hints: [count]
  - Shared state usage: [tools using it]
  - All tools have test cases: Yes/No
- Dependencies added to requirements.txt



================================================
FILE: .cursor/commands/add-mcp.md
================================================
# MCP Server Creation Task

Your task is to add Model Context Protocol (MCP) server to an Agency Swarm agent. MCP servers expose external tools and data sources to agents through a standardized protocol.

---

## Quick Reference: MCP Server Types

| Server Type               | When to Use                                | URL Pattern              | Class to Use                    |
| ------------------------- | ------------------------------------------ | ------------------------ | ------------------------------- |
| **Hosted Remote**         | Publicly accessible web service            | Any public URL           | `HostedMCPTool`                 |
| **Hosted Remote (OAuth)** | Publicly accessible web service with OAuth | Any public URL           | `MCPServerStdio` + `mcp-remote` |
| **Streamable HTTP**       | HTTP streaming server                      | Ends in `/mcp`           | `MCPServerStreamableHttp`       |
| **SSE**                   | Server-Sent Events server                  | Ends in `/sse`           | `MCPServerSse`                  |
| **Local/Stdio**           | GitHub repo, local script, or CLI tool     | GitHub URL or local path | `MCPServerStdio`                |

---

## Step-by-Step Process

### Step 1: Determine Server Type

**If user provides a URL:**

- Check if it's a public/internet-accessible URL:
  - **Requires OAuth authentication?** → Use `mcp-remote` with `MCPServerStdio`
  - **No authentication or token-based auth?** → Use `HostedMCPTool`
- Check if URL ends with `/mcp` → Use `MCPServerStreamableHttp` (or `HostedMCPTool` if public)
- Check if URL ends with `/sse` → Use `MCPServerSse` (or `HostedMCPTool` if public)

**If user provides a GitHub URL or mentions "local":**

- Use `MCPServerStdio`

**If unclear, ask:**

1. Is this a publicly accessible URL? (HostedMCPTool or mcp-remote)
2. Does it require OAuth authentication? (Use mcp-remote)
3. Is this a local server you'll run? (MCPServerStdio, MCPServerSse, or MCPServerStreamableHttp)
4. What's the server endpoint URL? (check `/mcp` or `/sse` suffix)
5. Which agent do you want to add this MCP server to?

---

### Step 2: Install and Setup Server (for Local/Stdio only)

**For GitHub repositories:**

1. Clone the repository to a local directory within your project:

```bash
git clone <repository-url> ./mcp_servers/<server-name>
```

2. Install dependencies (check the repo's README):

```bash
cd ./mcp_servers/<server-name>
# Common patterns:
npm install  # for Node.js servers
pip install -r requirements.txt  # for Python servers
uv sync  # for uv-based Python projects
```

3. Note the command needed to run the server (from README):

   - Node.js: `npx`, `node`, or `npm run`
   - Python: `python`, `uv run`, or `poetry run`

4. Check if API keys are needed and add them to `.env` file

**For existing npm packages:**

```bash
# No installation needed, use npx with -y flag
# Example: npx -y @modelcontextprotocol/server-filesystem
```

---

### Step 3: Add Server Configuration to Agent

Based on server type, add the appropriate configuration:

#### Option A: Hosted Remote Server (HostedMCPTool)

**When to use:** Server is publicly accessible on the internet.

```python
import os
from agency_swarm import Agent, HostedMCPTool

# Define the hosted MCP tool
hosted_mcp = HostedMCPTool(
    tool_config={
        "type": "mcp",
        "server_label": "descriptive-server-name",  # Choose descriptive name
        "server_url": "https://your-server.com/mcp/",  # or /sse/
        "require_approval": "never",  # or "always" for sensitive operations
        "headers": {
            "Authorization": f"Bearer {os.getenv('API_TOKEN_NAME')}"  # if auth required
        }
    }
)

# Add to agent's tools parameter (NOT mcp_servers)
agent = Agent(
    name="AgentName",
    description="Agent description",
    instructions="./instructions.md",
    tools=[hosted_mcp],  # Add to tools list
    model="gpt-5"
)
```

**Important:** HostedMCPTool goes in the `tools` parameter, not `mcp_servers`!

#### Option B: Local SSE Server (MCPServerSse)

**When to use:** Server runs locally or on private network with SSE transport.

```python
import os
from agency_swarm import Agent
from agents.mcp import MCPServerSse

# Define the SSE server
sse_server = MCPServerSse(
    name="Descriptive Server Name",
    params={
        "url": "http://localhost:8000/sse",  # Local or internal URL
        "headers": {
            "Authorization": f"Bearer {os.getenv('API_TOKEN_NAME')}",  # if needed
            "X-Custom-Header": "value"  # any custom headers
        }
    },
    cache_tools_list=True  # REQUIRED: Enable caching
)

# Add to agent's mcp_servers parameter
agent = Agent(
    name="AgentName",
    description="Agent description",
    instructions="./instructions.md",
    mcp_servers=[sse_server],  # Add to mcp_servers list
    model="gpt-5"
)
```

#### Option C: Local Streamable HTTP Server (MCPServerStreamableHttp)

**When to use:** Server uses HTTP POST with streaming responses.

```python
import os
from agency_swarm import Agent
from agents.mcp import MCPServerStreamableHttp

# Define the streamable HTTP server
http_server = MCPServerStreamableHttp(
    name="Descriptive Server Name",
    params={
        "url": "http://localhost:8000/mcp/",  # Usually ends in /mcp
        "headers": {
            "Authorization": f"Bearer {os.getenv('API_TOKEN_NAME')}"  # if needed
        }
    },
    cache_tools_list=True  # REQUIRED: Enable caching
)

# Add to agent's mcp_servers parameter
agent = Agent(
    name="AgentName",
    description="Agent description",
    instructions="./instructions.md",
    mcp_servers=[http_server],  # Add to mcp_servers list
    model="gpt-5"
)
```

#### Option D: Local Stdio Server (MCPServerStdio)

**When to use:** Server is a local script, executable, or needs to run as subprocess.

```python
import os
from pathlib import Path
from agency_swarm import Agent
from agents.mcp import MCPServerStdio

# Get the path to the local server
current_dir = Path(__file__).parent
path_to_stdio_mcp_server = current_dir / "mcp_servers" / "server-name"

# Define the stdio server
stdio_server = MCPServerStdio(
    name="Descriptive Server Name",
    params={
        "command": "uv",  # or "python", "node", "npx", etc.
        "args": [
            "--directory",
            str(path_to_stdio_mcp_server),
            "run",
            "server.py"  # Entry point from README
        ],
        "env": {
            # Pass required environment variables
            "API_KEY": os.getenv("API_KEY_NAME"),
            "ACCOUNT_ID": "your-account-id"  # if needed
        }
    },
    cache_tools_list=True,  # REQUIRED: Enable caching
    client_session_timeout_seconds=10,  # Adjust timeout as needed
    tool_filter={
        # Optional: Block specific tools you don't want to expose
        "blocked_tool_names": ["dangerous_tool", "unused_tool"]
        # Or allow only specific tools:
        # "allowed_tool_names": ["safe_tool1", "safe_tool2"]
    }
)

# Add to agent's mcp_servers parameter
agent = Agent(
    name="AgentName",
    description="Agent description",
    instructions="./instructions.md",
    mcp_servers=[stdio_server],  # Add to mcp_servers list
    model="gpt-5"
)
```

**Common command patterns for MCPServerStdio:**

```python
# NPM package (no installation needed)
params={
    "command": "npx",
    "args": ["-y", "@modelcontextprotocol/server-filesystem", "./files"]
}

# Local Python script with uv
params={
    "command": "uv",
    "args": ["--directory", str(server_path), "run", "server.py"]
}

# Local Python script with python
params={
    "command": "python",
    "args": [str(server_path / "server.py")]
}

# Local Node.js script
params={
    "command": "node",
    "args": [str(server_path / "index.js")]
}
```

#### Option E: Hosted Remote Server with OAuth (mcp-remote)

**When to use:** Server is publicly accessible but requires OAuth authentication (e.g., Notion, Google services).

This uses the `mcp-remote` tool ([documentation](https://github.com/geelen/mcp-remote)) to handle OAuth flows and token management for hosted MCP servers.

```python
import os
from pathlib import Path
from agency_swarm import Agent
from agents.mcp import MCPServerStdio

# Get the folder path to store MCP credentials locally
folder_path = Path(__file__).parent.parent  # Go up to agency root

# Define the OAuth MCP server using mcp-remote
oauth_mcp_server = MCPServerStdio(
    name="Descriptive Server Name",
    params={
        "command": "npx",
        "args": [
            "-y",
            "mcp-remote",
            "https://your-server.com/mcp"  # The hosted MCP server URL
        ],
        "env": {
            # Store OAuth credentials in ./mnt/mcp_credentials/ folder for persistence
            "MCP_REMOTE_CONFIG_DIR": os.path.join(folder_path, "mnt", "mcp_credentials")
        }
    },
    cache_tools_list=True,  # REQUIRED: Enable caching
    client_session_timeout_seconds=20,  # Increase timeout for OAuth flows
    tool_filter={
        # Optional: Limit to specific tools
        "allowed_tool_names": ["tool1", "tool2", "tool3"]
    }
)
```

**How it works:**

1. `mcp-remote` acts as a bridge between local stdio and remote OAuth servers
2. On first run, it will open a browser for OAuth authentication
3. Credentials are saved in the `mnt/mcp_credentials/` folder (don't add to `.gitignore`)
4. Subsequent runs reuse the saved tokens automatically. ./mnt folder is for persistent storage.

**Example of a Notion MCP Server:**

```python
# Notion MCP Server
notion_mcp = MCPServerStdio(
    name="Notion MCP",
    params={
        "command": "npx",
        "args": ["-y", "mcp-remote", "https://mcp.notion.com/mcp"],
        "env": {
            "MCP_REMOTE_CONFIG_DIR": os.path.join(folder_path, "mcp_credentials")
        }
    },
    cache_tools_list=True,
    client_session_timeout_seconds=20,
    tool_filter={
        "allowed_tool_names": ["notion-fetch", "notion-create-pages", "notion-update-page"]
    }
)
```

**Important Notes:**

- Increase `client_session_timeout_seconds` to at least 20 for OAuth flows
- The first run will require user interaction to authenticate
- Use `tool_filter` to limit exposed tools if needed

---

### Step 4: Add Required Environment Variables

1. Identify required API keys from server documentation
2. Add them to `.env` file:

```bash
# .env
OPENAI_API_KEY=your-key-here
API_TOKEN_NAME=your-token-here
YOUTUBE_API_KEY=your-key-here
# etc.
```

3. Ask user to fill in actual values

---

### Step 5: Update Agent Instructions

Update the agent's `instructions.md` file to provide clear, minimal guidance for using the new MCP server. Do not alter or remove existing user-written instructions—only add what is necessary to instruct the agent in referencing or using the MCP server by its name within its step-by-step process or task list. Ensure the MCP server name is explicitly mentioned in one or more steps so the agent is aware of its presence and capabilities. The agent will automatically have access to the MCP server tools and does not need to be instructed to find or import them.

---

### Step 6: Test the MCP Server Integration

Add a test block to the agent file to verify MCP tools are accessible:

```python
# At the bottom of agent_name.py file

if __name__ == "__main__":
    import asyncio

    async def test_agent():
        print("Testing MCP server integration...")

        # List available tools (should include MCP tools)
        if hasattr(agent, 'mcp_servers'):
            for server in agent.mcp_servers:
                await server.connect()
                tools = await server.list_tools()
                print(f"\nTools from {server.name}:")
                for tool in tools:
                    print(f"  - {tool.name}: {tool.description}")

        # Test with a sample message
        result = await agent.get_response("List the available tools you have access to")
        print(f"\nAgent response:\n{result.final_output}")

        # Test actual tool usage (customize based on available tools)
        # result = await agent.get_response("Use [tool_name] to [perform action]")
        # print(f"\nTool test response:\n{result.final_output}")

    asyncio.run(test_agent())
```

**Run the test:**

```bash
python agent_name/agent_name.py
```

**Expected output:**

- List of MCP tools should appear
- Agent should confirm access to the tools
- No errors about missing tools or connection issues

---

## Common MCP Server Examples

### 1. Notion Server (OAuth-authenticated Hosted)

```python
import os
from pathlib import Path
from agents.mcp import MCPServerStdio

folder_path = Path(__file__).parent.parent

notion_mcp = MCPServerStdio(
    name="Notion_MCP",
    params={
        "command": "npx",
        "args": ["-y", "mcp-remote", "https://mcp.notion.com/mcp"],
        "env": {
            "MCP_REMOTE_CONFIG_DIR": os.path.join(folder_path, "mnt", "mcp_credentials")
        }
    },
    cache_tools_list=True,
    client_session_timeout_seconds=20,
    tool_filter={
        "allowed_tool_names": ["notion-fetch", "notion-create-pages", "notion-update-page"]
    }
)
```

### 2. Filesystem Server (Local)

```python
from pathlib import Path
from agents.mcp import MCPServerStdio

samples_dir = Path(__file__).parent / "files"

filesystem_server = MCPServerStdio(
    name="Filesystem_Server",
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", str(samples_dir)]
    },
    cache_tools_list=True
)
```

### 3. GitHub Server (Hosted with Token Auth)

```python
from agency_swarm import HostedMCPTool
import os

github_mcp = HostedMCPTool(
    tool_config={
        "type": "mcp",
        "server_label": "github-server",
        "server_url": "https://github-mcp-server.com/mcp/",
        "require_approval": "never",
        "headers": {
            "Authorization": f"Bearer {os.getenv('GITHUB_TOKEN')}"
        }
    }
)
```

### 4. Database Server (Local SSE)

```python
from agents.mcp import MCPServerSse
import os

db_server = MCPServerSse(
    name="Database_Server",
    params={
        "url": "http://localhost:8000/sse",
        "headers": {
            "X-Database": "production"
        }
    },
    cache_tools_list=True
)
```

---

## Tool Filtering

Limit which tools the agent can access using filters:

```python
# Block specific tools
stdio_server = MCPServerStdio(
    name="Server_Name",
    params={...},
    cache_tools_list=True,
    tool_filter={
        "blocked_tool_names": ["delete_all", "dangerous_operation"]
    }
)

# Or allow only specific tools
stdio_server = MCPServerStdio(
    name="Server_Name",
    params={...},
    cache_tools_list=True,
    tool_filter={
        "allowed_tool_names": ["read_file", "write_file", "list_files"]
    }
)
```

---

## Troubleshooting

If there is an issue, you must keep troubleshooting until the MCP server is working as expected. Common problems:

### Server won't start (MCPServerStdio)

1. Check the command is correct: `python`, `node`, `npx`, `uv`, etc.
2. Verify the path to the server exists
3. Check dependencies are installed
4. Look for error output in console
5. Try running the command manually first

### Tools not appearing

1. Verify you called `await server.connect()` (for stdio servers)
2. Check server is running (for SSE/HTTP servers)
3. Try calling `await server.list_tools()` manually
4. Check authentication headers if required

### Authentication errors

1. Verify API keys are in `.env` file
2. Ensure `.env` file is loaded with `load_dotenv()`
3. For remote MCP servers that require OAuth, use mcp-remote npm package:

```json
{
  "mcpServers": {
    "remote-example": {
      "command": "npx",
      "args": ["mcp-remote", "https://remote.mcp.server/mcp"]
    }
  }
}
```

Fetch this link for more details: https://raw.githubusercontent.com/geelen/mcp-remote/refs/heads/main/README.md

### Timeout errors

1. Increase `client_session_timeout_seconds` parameter
2. Check server is responsive
3. Verify network connectivity (for remote servers)

### OAuth / mcp-remote errors

1. **Timeout during OAuth**: Increase `client_session_timeout_seconds` to 30 or higher
2. **"Client mode" debugging**: Test connection independently:
   ```bash
   npx -y mcp-remote https://your-server.com/mcp
   ```
3. **Check logs**: Enable debug logging with `--debug` flag:
   ```python
   params={
       "command": "npx",
       "args": ["-y", "mcp-remote", "https://your-server.com/mcp", "--debug"]
   }
   ```

---

## Important Configuration Notes

1. **Always set `cache_tools_list=True`** - Required for performance
2. **HostedMCPTool uses `tools` parameter** - Not `mcp_servers`
3. **All servers except HostedMCPTool use `mcp_servers` parameter** - Not `tools`
4. **Import `load_dotenv()` and call it** - Before accessing environment variables
5. **Use descriptive server names** - Agent sees `[Server_Name].[tool_name]` format
6. **Test after adding** - Always verify tools are accessible
7. **OAuth timeout** - Set `client_session_timeout_seconds` to at least 20 for OAuth flows
8. **Allowed Tools** - `tool_filter` parameter only works when running an agent, not list_tools() method.
9. **Persistent Storage** - Use `mnt` folder for persistent storage of OAuth credentials and other data.

---

## References

- [Agency Swarm MCP Documentation](https://agency-swarm.ai/core-framework/tools/mcp-integration)
- [OpenAI Agents SDK MCP Guide](https://openai.github.io/openai-agents-python/mcp/)
- [MCP Protocol Specification](https://modelcontextprotocol.io/)
- [MCP Servers Directory](https://github.com/modelcontextprotocol/servers)
- [mcp-remote - OAuth bridge for MCP servers](https://github.com/geelen/mcp-remote)

---

## Quick Decision Tree

```
User wants to add MCP server
│
├─ Is it a public URL?
│  ├─ Requires OAuth? → Use mcp-remote with MCPServerStdio (in mcp_servers)
│  │  ├─ Use npx -y mcp-remote <URL>
│  │  ├─ Set MCP_REMOTE_CONFIG_DIR to local folder
│  │  └─ First run opens browser for OAuth
│  └─ No OAuth? → Use HostedMCPTool (in tools parameter)
│
├─ Is it a local server URL?
│  ├─ Ends in /sse? → Use MCPServerSse (in mcp_servers)
│  └─ Ends in /mcp? → Use MCPServerStreamableHttp (in mcp_servers)
│
└─ Is it a GitHub repo or local script?
   └─ YES → Use MCPServerStdio (in mcp_servers)
      ├─ Clone repo to ./mcp_servers/<name>
      ├─ Install dependencies
      ├─ Configure command and args
      └─ Test with python agent_name.py
```



================================================
FILE: .cursor/commands/create-prd.md
================================================
# PRD Creation - Product Requirements Document for Agency Swarm

Your task is to create a Product Requirements Document (PRD) that defines the agency structure, agent roles, and tool specifications.

---

## Step 1: Ask Questions

Ask the user for:

- **Agency Name**
- **Purpose**: What does the agency do?
- **Agent Roles**: What roles are needed?
- **Communication Flows**: How do agents communicate?
- **Tools per Agent**: What actions should each agent perform?
- **External Integrations**: What APIs or services will be used?

---

## Step 2: Research

Search for available MCP servers for the systems user wants to connect to.

**MCP Search Priority:**

1. Check official registry: https://github.com/mcp
2. Search using web search: `mcp server <system name>`
3. If not MCP server is found, search for API documentation and create a custom tool that wraps the API call.

**Common MCP Servers:**

- `@modelcontextprotocol/server-filesystem` - File operations
- `@modelcontextprotocol/server-github` - GitHub
- `@modelcontextprotocol/server-slack` - Slack
- `@modelcontextprotocol/server-postgres` - PostgreSQL
- `@modelcontextprotocol/server-sqlite` - SQLite
- `@modelcontextprotocol/server-puppeteer` - Web automation
- `@modelcontextprotocol/server-brave-search` - Web search

**Built-in Tools:**

- `WebSearchTool` - Built-in web search (use this instead of MCP for web search)

**For each integration:**

- Document MCP server package name if found
- Note required API keys and how to obtain them
- If no MCP exists, note that custom tool will be needed

---

## Step 3: Create PRD

Create `./prd.txt` using this template:

```md
# [Agency Name]

---

- **Purpose:** [High-level description: what the agency achieves, target market, value proposition]

- **Communication Flows:**
  - **Between Agents:**
    - [Description of protocols and flows between agents, shared resources, data exchange]
    - **Example Flow:**
      - **[Agent A] -> [Agent B]:** [Trigger conditions, interaction description, expected outcomes]
      - **[Agent B] -> [Agent C]:** [Trigger conditions, interaction description, expected outcomes]
  - **Agent to User Communication:** [How agents communicate with users, interfaces, channels]

---

## [Agent Name]

### Role within the Agency

[Description of realistic job role and responsibilities - model after actual professions]

### Tools

- **[ToolName]:**
  - **Description**: [What this tool does and when it's used - must be a real-world task]
  - **Inputs**:
    - [name] (type) - description
  - **Validation**:
    - [Condition] - description
  - **Core Functions:** [Main functions the tool performs]
  - **APIs**: [List of APIs used, if any]
  - **Output**: [Expected output format - string or JSON object]
- **MCP Name**: [MCP server package name if found]
  - **URL**: [Url of the server’s github / official page]
  - **Authentication**: [How the MCP is authenticated]
  - **Allowed Tools**: [Which tools to allow]

---

[...repeat for each agent]
```

---

## Best Practices

**Agent Design:**

- Start with 1 agent
- Only add more if user explicitly requests
- Model after real jobs: "Data Analyst" ✅, not "Chart Creator" ❌

**Tool Design Best Practices:**

Each tool should perform one simple, human-like action (e.g., "FetchInstagramLeads" ✅, not "OptimizeText" ❌). Key characteristics of tools:

- **Standalone:** Tools must run independently with minimal dependencies on other tools or agents. Any agent should be able to use any tool without requiring additional prompting or coordination.
- **Configurable:** Tools should expose adjustable parameters (such as modes, thresholds, timeouts, limits, etc.) so agents can tune them to suit different environments or task requirements.
- **Composable:** The output format of each tool should match the input format of others wherever possible. This enables agents to autonomously chain tools together into workflows rather than relying on rigid, pre-defined sequences.

Tools can be either:

- MCP server
- Custom tool that wraps an API call
- Built-in tool (e.g., `WebSearchTool`)
- Custom tool that performs a specific action on local files (e.g., `IPythonInterpreter`, `LocalShell`, `WriteFile`)

**API Requirements:**

- List all required API keys for `.env.template` file
- Prefer MCP servers for common platforms
- If no MCP server is found, create a custom tool that wraps an API call.

---

## References

- [Agency Swarm Documentation](https://agency-swarm.ai/llms.txt)
- [Workflow Guide](.cursor/rules/workflow.mdc)



================================================
FILE: .cursor/commands/mcp-code-exec.md
================================================
# MCP Code Execution Pattern - Implementation Guide

Your task is to convert an MCP server into the **Code Execution Pattern** described in [Anthropic's blog post](https://www.anthropic.com/engineering/code-execution-with-mcp). This pattern enables **progressive disclosure** of tools, reducing token usage by up to 98% compared to loading all tools upfront.

---

## Why This Pattern?

**Problem with Traditional MCP:**

- Loading all tools upfront consumes 150,000+ tokens
- Intermediate tool results flow through model context repeatedly
- Agents slow down with many connected tools

**Solution - Code Execution Pattern:**

- Tools organized as importable Python modules (filesystem-based)
- Agents load only what they need (progressive disclosure)
- Data processing happens in code before returning to model
- **Result**: 98.7% token reduction (150K → 2K tokens)

---

## Step-by-Step Implementation

### Step 1: Create Folder Structure

Create a dedicated folder for the MCP server in `./servers/` directory:

```
./servers/[server_name]/
├── server.py           # Server singleton & connection management (with discovery in __main__)
├── [tool_1].py         # Individual tool file (with test in __main__)
├── [tool_2].py         # Individual tool file (with test in __main__)
├── [tool_3].py         # Individual tool file (with test in __main__)
└── __init__.py         # Package exports
```

**Example:**

```
./servers/notion/
├── server.py
├── notion_search.py
├── notion_fetch.py
├── notion_create_pages.py
└── __init__.py
```

**Create the folder:**

```bash
mkdir -p ./servers/[server_name]
```

---

### Step 2: Create the Server Module

The `server.py` module manages the MCP server connection as a singleton. Include tool discovery in the `__main__` block to list available tools.

**Note:** This pattern works with **any MCP server type**. The template below shows `MCPServerStdio` (most common), but you can use:

- `MCPServerStdio` - Local scripts, npm packages, or OAuth servers with `mcp-remote`
- `MCPServerSse` - Server-Sent Events servers
- `MCPServerStreamableHttp` - HTTP streaming servers
- `HostedMCPTool` - Publicly accessible servers (no need for singleton pattern)

See `.cursor/commands/add-mcp.md` for detailed configuration of each server type.

**Template (`./servers/[server_name]/server.py`):**

```python
"""[Server Name] MCP Server Configuration"""
# Choose the appropriate import based on your server type:
from agents.mcp import MCPServerStdio  # For local scripts, npm packages, or mcp-remote
# from agents.mcp import MCPServerSse  # For Server-Sent Events servers
# from agents.mcp import MCPServerStreamableHttp  # For HTTP streaming servers
# from agency_swarm import HostedMCPTool  # For publicly accessible servers (different pattern)

from typing import Optional

# Singleton server instance
_server: Optional[MCPServerStdio] = None  # Adjust type if using different server
_connected: bool = False

def get_server() -> MCPServerStdio:  # Adjust return type if using different server
    """Get the [Server Name] MCP server instance (singleton)"""
    global _server
    if _server is None:
        # Example: Stdio server with mcp-remote (OAuth)
        _server = MCPServerStdio(
            name="[Server_Name]",
            params={
                "command": "npx",  # or "python", "node", etc.
                "args": ["-y", "mcp-remote", "https://your-server.com/mcp"],
                # Add env vars if needed:
                # "env": {
                #     "API_KEY": os.getenv("API_KEY_NAME")
                # }
            },
            cache_tools_list=True,
            client_session_timeout_seconds=30  # Increase for OAuth
        )

        # Example: SSE server
        # _server = MCPServerSse(
        #     name="[Server_Name]",
        #     params={
        #         "url": "http://localhost:8000/sse",
        #         "headers": {"Authorization": f"Bearer {os.getenv('API_TOKEN')}"}
        #     },
        #     cache_tools_list=True
        # )

        # Example: Remote HTTP server
        # _server = HostedMCPTool(
        #     name="[Server_Name]",
        #     tool_config={
        #         "type": "mcp",
        #         "server_label": "[Server_Name]",
        #         "server_url": "https://your-server.com/mcp",
        #         "require_approval": "never",
        #         "headers": {
        #             "Authorization": f"Bearer {os.getenv('API_TOKEN')}"
        #         }
        #     }
        # )

        # Example: Remote OAuth server
        # _server = MCPServerStdio(
        #     name="[Server_Name]",
        #     params={
        #         "command": "npx",
        #         "args": ["-y", "mcp-remote", "https://your-server.com/mcp"],
        #         "env": {
        #             "MCP_REMOTE_CONFIG_DIR": os.path.join(folder_path, "mnt", "mcp-creds") # persistent storage for OAuth credentials, don't add to .gitignore
        #         }
        #     },
        #     cache_tools_list=True,
        #     client_session_timeout_seconds=30
        # )
    return _server

async def ensure_connected() -> MCPServerStdio:  # Adjust return type if using different server
    """Ensure the server is connected, connect if not"""
    global _connected
    server = get_server()

    if not _connected:
        await server.connect()
        _connected = True

    return server

async def call_tool(tool_name: str, arguments: dict):
    """Call a tool on the MCP server with the given arguments"""
    server = await ensure_connected()

    # Get the session
    session = getattr(server, 'session', None) or getattr(server, '_client_session', None)
    if not session:
        raise RuntimeError("Could not access MCP session")

    # Call the tool
    result = await session.call_tool(name=tool_name, arguments=arguments)

    # Extract content from result
    if hasattr(result, 'content'):
        content = result.content
        if isinstance(content, list) and len(content) > 0:
            return content[0].text if hasattr(content[0], 'text') else str(content[0])
        return str(content)

    return result

# Tool discovery - run with: python ./servers/[server_name]/server.py
if __name__ == "__main__":
    import asyncio

    async def discover_tools():
        print("Discovering tools from MCP server...")
        server = await ensure_connected()

        session = getattr(server, 'session', None) or getattr(server, '_client_session', None)
        if session:
            tools_result = await session.list_tools()
            tools = tools_result.tools if hasattr(tools_result, 'tools') else []

            print(f"\nFound {len(tools)} tools:\n")
            for tool in tools:
                print(f"Tool: {tool.name}")
                print(f"  Description: {tool.description}")
                if hasattr(tool, 'inputSchema') and 'properties' in tool.inputSchema:
                    print(f"  Parameters:")
                    schema = tool.inputSchema
                    for param_name, param_info in schema['properties'].items():
                        param_type = param_info.get('type', 'any')
                        param_desc = param_info.get('description', '')
                        required = param_name in schema.get('required', [])
                        print(f"    - {param_name}: {param_type} {'(required)' if required else '(optional)'}")
                        if param_desc:
                            print(f"      {param_desc}")
                print()

    asyncio.run(discover_tools())
```

**Run discovery:**

```bash
python ./servers/[server_name]/server.py
```

This will list all available tools with their descriptions and parameters. Use this output to create individual tool files.

---

### Step 3: Create Individual Tool Files

For each tool discovered by running `server.py`, create a dedicated Python file. **Use the exact descriptions and parameters from the MCP server output** - don't add examples or extra documentation unless provided by the MCP server.

**Naming Convention:**

- File name: `[mcp_prefix]_[tool_name].py` (e.g., `notion_fetch.py`, `gdrive_search.py`)
- Function name: Must match file name without `.py` (e.g., `notion_fetch()`, `gdrive_search()`)

**Template (`./servers/[server_name]/[mcp_prefix]_[tool_name].py`):**

```python
from typing import Optional, Dict, List, Any
from .server import call_tool


async def [mcp_prefix]_[tool_name](
    required_param: str,
    optional_param: Optional[str] = None,
    another_param: Optional[Dict[str, Any]] = None
) -> str:
    """
    [Tool description from MCP server]

    Args:
        required_param: [Parameter description from MCP]
        optional_param: [Parameter description from MCP]
        another_param: [Parameter description from MCP]

    Returns:
        Tool result as string
    """
    # Build arguments dict with required params
    arguments = {"required_param": required_param}

    # Add optional params only if provided
    if optional_param:
        arguments["optional_param"] = optional_param
    if another_param:
        arguments["another_param"] = another_param

    # Call the MCP tool
    return await call_tool("[mcp-tool-name]", arguments)


# IMPORTANT: If MCP server schema shows a single 'data' or 'options' parameter,
# document the expected dict structure in the docstring based on the examples:
async def [mcp_prefix]_[tool_with_data_param](data: Dict[str, Any]) -> str:
    """
    [Tool description from MCP server]

    Args:
        data: The data required for the operation. Should contain:
            - field1 (str, required): Description from MCP examples
            - field2 (str, optional): Description from MCP examples
            - field3 (dict, optional): Description from MCP examples

    Example from MCP server:
        {
            "field1": "value",
            "field2": "optional_value",
            "field3": {"nested": "data"}
        }

    Returns:
        Tool result as string
    """
    arguments = {"data": data}
    return await call_tool("[mcp-tool-name]", arguments)


# Test - run with: python ./servers/[server_name]/[mcp_prefix]_[tool_name].py
if __name__ == "__main__":
    import asyncio

    async def test():
        print("Testing [mcp_prefix]_[tool_name]...")
        try:
            result = await [mcp_prefix]_[tool_name]("test_value")
            print(f"✓ Success: {str(result)[:200]}...")
        except Exception as e:
            print(f"✗ Error: {e}")

    asyncio.run(test())
```

---

### Step 4: Create Package Exports

The `__init__.py` file makes tools easily importable.

**Template (`./servers/[server_name]/__init__.py`):**

```python
"""
[Server Name] MCP Tools

Progressive disclosure pattern - import only what you need.
See: https://www.anthropic.com/engineering/code-execution-with-mcp
"""

# Server management
from .server import get_server, ensure_connected, call_tool

# Individual tools
from .[mcp_prefix]_[tool_1] import [mcp_prefix]_[tool_1]
from .[mcp_prefix]_[tool_2] import [mcp_prefix]_[tool_2]
from .[mcp_prefix]_[tool_3] import [mcp_prefix]_[tool_3]
# ... more imports

__all__ = [
    # Server
    "get_server",
    "ensure_connected",
    "call_tool",
    # Tools
    "[mcp_prefix]_[tool_1]",
    "[mcp_prefix]_[tool_2]",
    "[mcp_prefix]_[tool_3]",
    # ... more exports
]
```

Also, add nest_asyncio to requirements.txt.

```
nest_asyncio
```

This helps the AI agent to use async MCP calls inside IPython without conflicts.

---

### Step 5: Test the Implementation

Test the implementation by running specific tool files using the following command:

```bash
python ./servers/[server_name]/[mcp_prefix]_[tool_name].py
```

Only execute read tools, do not update, or create any data. Make sure you do not make any changes to the user's accounts.

**Do not come back to the user until you have tested at least 1 tool for each MCP server.**

### Step 6: Update instructions.md

Update instructions.md to reflect the new MCP server implementation. The agent should follow this process:

1. Discover existing skills in ./mnt/skills folder
2. Use skill if it matches task
3. If no skills found, read ONLY necessary tool files
4. Import and combine tools in IPythonInterpreter
5. Suggest new skills to be added

Keep these instructions short. Don't add mcp usage examples or don't list all mcps. Agent should discover them autonomously.

Agent should also minimize token consumption by performing as few tool calls as possible and only reading the necessary tool files to complete the task.

Perform minimal changes to instructions in order to achieve desired behavior.

## Troubleshooting

### Tools not connecting

**Symptom**: `RuntimeError: Could not access MCP session`

**Fix**:

1. Check server.py is correctly implemented
2. Verify session access: try both `server.session` and `server._client_session`
3. Ensure `await ensure_connected()` is called before tool use

### OAuth timeout (Important!)

**Symptom**: `McpError: Timed out while waiting for response`

**Fix**: Increase `client_session_timeout_seconds` to 30 or higher in server.py

### User can't authenticate on deployed server

**Symptom**: MCP server can't authenticate when agency is deployed.

**Fix**:

1. Ensure `MCP_REMOTE_CONFIG_DIR` is set correctly for mcp-remote servers.
2. Ensure all other MCP servers store credentials in `./mnt/mcp-creds`.
3. Do not add credentials to .gitignore or .dockerignore. Instead, tell the user to ensure their repo is not public.
4. Make sure to add node install to Dockerfile if using any npm servers, like mcp-remote.

---

## Quick Checklist

- [ ] Create `./servers/[server_name]/` folder
- [ ] Implement `server.py` with singleton pattern and discovery in `__main__`
- [ ] Run `python ./servers/[server_name]/server.py` to discover tools
- [ ] Create individual tool files using descriptions from MCP server output
- [ ] All OAuth servers store credentials in `./mnt/mcp-creds` for persistence.
- [ ] Node install is added to Dockerfile if using any npm servers.
- [ ] nest_asyncio is added to requirements.txt.
- [ ] Add `if __name__ == "__main__"` test blocks to each tool file
- [ ] Create `__init__.py` with exports
- [ ] Test individual tools: `python ./servers/[server_name]/[mcp_prefix]_[tool].py`
- [ ] Update instructions.md to ensure the agent can effectively use these tools.

---

## References

- [Anthropic: Code execution with MCP](https://www.anthropic.com/engineering/code-execution-with-mcp)
- [Agency Swarm MCP Integration](https://agency-swarm.ai/core-framework/tools/mcp-integration)
- [Adding MCP Servers to Agents](.cursor/commands/add-mcp.md)

## Final Notes

- You MUST create **all tools** for each MCP server. Tools must never contain placeholders. Each tool must be a production-ready functional code.
- **CRITICAL**: When MCP schema shows a single `data`, `options`, or similar dict parameter, you MUST document the expected dict structure in the docstring using the examples from the MCP tool description. Never leave dict parameters undocumented.
- If credentials for testing are missing, notify the user to provide them before starting to create the tools or any MCP server.
- YOU MUST NEVER SKIP THE TEST. NEVER CREATE A DUMMY TEST CASE WITH ONLY PRINT STATEMENTS OR IMPORTS. ASK THE USER TO PROVIDE THE CREDENTIALS INSTEAD.
- DO NOT tell the user the task has been completed until you have created **all tools** and tested at least 1 for each MCP server.



================================================
FILE: .cursor/commands/productize.md
================================================
# Agent Productization Task

Your task is to productize an AI agent in the current repository by turning it into a reusable template. Essentially, the user wants you to ensure that he can easily customize this agent for different clients.

## Step-by-Step Process

1. Fetch the documentation for how to create onboarding forms for AI agents with our framework: https://agency-swarm.ai/platform/marketplace/onboarding Make sure to fetch the entire page.
2. Explore the current repository to understand the structure of the current agency to productize. Focus primarily on insturctions files of each agent and the shared_instructions.md file.
3. Ask the user what he would like to customize accross different clients for this agent. For example, agent name, business overview, model, output format, additional notes, etc.
4. Based on the user's answers, create a new OnboardingTool for an agent, as described in documentation. Example is provided below.
5. **Important**: When converting the agent to use an OnboardingTool, make sure to preserve existing values from the repository.
   - For each value (for example, business overview or server addresses) that you want to turn into a customizable field, do NOT overwrite or delete the user’s current data.
   - Instead, create a field for this value in the OnboardingTool, and store the existing value as the default in `onboarding_config.py`.
   - Place code that loads these defaults inside the `if __name__ == "__main__"` block of your OnboardingTool script.
   - This approach keeps the user’s current settings safe while making them configurable for future onboarding.
6. Run the onboarding_tool.py file to generate the onboarding_config.py file.
7. Import the onboarding_config.py file to customize the agent, as described in documentation.

## Example OnboardingTool

```python
from agency_swarm.tools import BaseTool
from pydantic import Field
from typing import Optional, Literal
import os
import json

class OnboardingTool(BaseTool):
    """
    Customizes the agent based on business requirements and preferences.
    Add any fields you want the user to configure during onboarding.
    """

    # Basic text field example
    company_name: str = Field(
        "Acme Corp",
        description="Your company name"
    )

    # Textarea field example (for longer text inputs)
    company_overview: str = Field(
        "A company that does amazing things.",
        description="Brief overview of your company or product",
        json_schema_extra={"ui:widget": "textarea"},
    )

    model: Literal["gpt-5", "gpt-4.1"] = Field(
        "gpt-4.1",
        description="Select the model to use: gpt-5 or gpt-4.1"
    )

    # File upload field example
    knowledge_files: list[str] = Field(
        [],
        description="Upload documentation files for the agent to reference.",
        json_schema_extra={
            "x-file-upload-path": "./agent_name/files",  # Replace with your agent folder. FIles will be automatically uploaded to this path.
        },
    )

    # Add more fields as needed:
    # - Use str for text inputs
    # - Use Optional[str] for optional inputs
    # - Use list[str] for file uploads
    # - Add json_schema_extra={"ui:widget": "textarea"} for multi-line text
    # - Add json_schema_extra={"ui:placeholder": "placeholder"} for field placeholder text
    # - Add json_schema_extra={"x-file-upload-path": "./path"} for file uploads

    def run(self):
        """Saves configuration to onboarding_config.py"""
        # Get the directory where this tool is located
        tool_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(tool_dir, "onboarding_config.py")

        # Convert tool fields to dictionary
        config = self.model_dump()

        # Convert to Python code format
        json_str = json.dumps(config, indent=4)
        json_str = json_str.replace(': null', ': None').replace(': true', ': True').replace(': false', ': False')
        python_code = f"# Auto-generated configuration\n\nconfig = {json_str}\n"

        # Write to file
        with open(config_path, "w", encoding="utf-8") as f:
            f.write(python_code)

        return f"Configuration saved to {config_path}"

# Test the tool
if __name__ == "__main__":
    tool = OnboardingTool(
        company_name="Test Company",
        company_overview="This is a test.",
        support_email="support@test.com"
    )
    print(tool.run())
```

**Key Points:**

- Each field becomes a form input in the onboarding UI
- Use clear descriptions - they become field labels/help text
- The `run()` method saves all fields to `onboarding_config.py`
- Import the config in your agent's instructions or tools with: `from .tools.onboarding_config import config`

## Example Usage

After running the OnboardingTool, use the generated config in your agent:

```python
from agency_swarm import Agent
from .tools.onboarding_config import config
import os

# Option 1: Use config values directly in agent initialization
my_agent = Agent(
    name=config["company_name"] + " Assistant",
    description="Helps with customer inquiries",
    instructions="./instructions.md",
    files_folder="./files",
    model="gpt-5"
)

# Option 2: Dynamically render instructions with config values
def render_instructions():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    instructions_path = os.path.join(current_dir, "instructions.md")

    with open(instructions_path, "r") as file:
        instructions = file.read()

    # Use Python's format() to inject config values into instructions
    instructions = instructions.format(
        company_name=config["company_name"],
        company_overview=config["company_overview"],
        support_email=config.get("support_email", "N/A")
    )

    return instructions

my_agent = Agent(
    name="Support Agent",
    description="Customer support assistant",
    instructions=render_instructions(),  # Dynamic instructions
    files_folder="./files",
    model=config["model"], # example how to customize the model.
)
```

In your `instructions.md` file, use placeholders:

```markdown
# Role

You are a customer support agent for {company_name}.

# Company Overview

{company_overview}

# Contact Information

For escalations, contact: {support_email}
```

## Best Tips & Practices

- Handle None values gracefully in your code. If the field is optional, use the `get` method to get the value, and provide a default value if the value is None.



================================================
FILE: .cursor/commands/write-instructions.md
================================================
# Agent Instructions Writer - Implementation Guide

YOu are a professional prompt engineer. Your goal is to achieve the desired agent behaviour by writing or refining agent prompts.

---

## Core Principles

1. **Start Simple**: Use concise, verb-driven instructions
2. **Be Specific**: Explicitly state desired outputs and formats
3. **Provide Examples**: Include concrete examples of expected behavior
4. **Use Positive Instructions**: "Do this" rather than "Don't do that"
5. **Integrate Tools**: Show exactly when and how to use each tool
6. **Test Your Own Writing**: Think about how you would react to the instructions you're writing
7. **Minimal Changes**: When refining, make the smallest change needed to fix the issue

---

## Instructions Template

```markdown
# Role

**[Role, e.g., "Data analyst for financial reports"]**

# Goals

- [High level goal 1 - e.g., "increase sales by 10%"]
- [High level goal 2 - e.g., "improve customer satisfaction"]

# Context

- Part of: [agency]
- Works with: [other agents]
- Used for: [purpose]

# Examples (Optional - if provided by the user)

**1. [Scenario]**  
Input: "[sample]"  
Process: [steps, tools, validation]  
Output: "[expected]"

**2. [Edge Case]**  
Input: "[error or odd case]"  
Process: [detection, recovery, notify]  
Output: "[error response]"

# Instructions

## [Task Name]

**[Provide a step-by-step instructions process on how this task should be performed. Use a numbered list. Include specific tools in steps.]**

[...repeat for each task]

# Additional Notes

- [Any additional notes that don't fit into any of the other sections, or reiterate on the most critical details]
```

---

## Process

**Step 1: Explore & Understand**

- Check folder structure: `agency.py`, agent folders, `shared_instructions.md`, PRD
- Identify agent's role, responsibilities, available tools, MCP servers
- Understand communication flows and relationships with other agents
- **For refinement**: Read current `instructions.md` FIRST

**Step 2: Ask the User Key Questions**

- What are the main goals for this agent?
- What process should the agent follow?
- What specific tasks does the agent need to complete?
- What is your preferred output format?
- Do you have any examples you can privde?
- Any other additional notes?

**Step 3: Identify What to Write/Fix**

- **New agent**: Define role, tasks, workflow from PRD/docs and user responses
- **Refinement**: What's failing? Why? (tool usage, logic, format, performance)

**Step 4: Write/Modify Instructions**

- **New agent**: Use template above - leave sections blank if no information
- **Refinement**: Make MINIMAL changes - modify existing steps, don't add unless necessary
- Integrate tools with specific parameters and conditions
- Include error handling

**Step 5: Self-Check**

- Would YOU understand and follow these instructions?
- Are tool parameters, conditions, and output formats explicit?
- **For refinement**: Is this the smallest change that fixes the issue?

**Critical Rules**:

- **No speculation** - leave blank if you don't have information
- **Examples are optional** - skip if not provided
- **Only write what you know** - don't make up tool names, parameters, or workflows
- **Minimal changes** - make the smallest change needed to achieve a desired behavior of the agent.

---

## After Completion

Report back with a brief summary:

- Agent(s) updated: [names and paths]
- Changes made: [what was modified/created]
- Ready for: [testing/next phase]

---

## References

- [Agency Swarm Documentation](https://agency-swarm.ai/llms.txt)



================================================
FILE: .cursor/rules/workflow.mdc
================================================
---
description: AI Agent Creator Instructions for Agency Swarm Framework
alwaysApply: true
---

Agency Swarm is the framework built on the OpenAI Agents SDK. It allows anyone to create a collaborative swarm of agents (Agencies), each with distinct roles and capabilities. Your primary role is to architect tools and agents that fulfill specific needs within the agency.

The following steps outline how to build AI agents with Agency Swarm:

0. **Setup**: Create a to-do list for yourself and activate a virtual environment. If virtual environment does not exist, create it.
1. **Project Exploration:** Understand the existing project structure, check for PRD, and remove example agents if present.
2. **Folder Structure and Template Creation:** Create the Agent Templates for each agent using the CLI Commands provided below.
3. **Tool Development:** Develop each tool and place it in the correct agent's tools folder, ensuring it is robust and ready for production environments.
4. **Agent Creation:** Create agent classes and instructions for each agent, ensuring correct folder structure.
5. **Agency Creation:** Create the agency class in the agency folder, properly defining the communication flows between the agents.
6. **Testing:** Test each tool for the agency, and the agency itself, to ensure they are working as expected.
7. **Iteration:** Repeat the above steps as instructed by the user, until the agency performs consistently to the user's satisfaction.

You will find a detailed guide for each of the steps below. Read this entire file first before proceeding.

# Step 1: Project Exploration

Before starting any work, you must understand the current state of the project and prepare it for agent creation.

## Exploration Checklist

1. **Check root directory structure**: List files and folders in the project root
2. **Look for PRD**: Check if `prd.txt` exists in the root directory
3. **Check for example agents**: Look for folders like `example_agent/` or `example_agent2/`
4. **Review existing files**: Read `agency.py`, `shared_instructions.md`, `agent_name/instructions.md`, `agent_name/tools/`, etc.

## Actions Required

### If PRD exists:

- Read the entire `prd.txt` file to understand the agency structure
- Verify agent roles are realistic (modeled after real job positions)
- Confirm tool specifications are clear and actionable
- Proceed to Step 2 (Folder Structure)

### If PRD does not exist:

- **Proceed directly with user instructions**

### If example agents exist:

- **Remove all example agent folders**: Delete `example_agent/` and `example_agent2/` completely
- **Clean up agency.py**: Remove example agent imports and communication flows

### If the task required connecting to external systems (slack, notion, etc):

- Check if the user has added the API keys to the .env file.
- If not, ask the user to add the API keys before proceeding with the task.
- Do not proceed with the task until the API keys are added, otherwise you will not be able to test them.

# Step 2: Folder Structure and Template Creation

After exploration, setup the folder structure for each agent.

The basic folder structure is already created for you:

```
├── example_agent/
│   ├── __init__.py
│   ├── example_agent.py
│   ├── instructions.md
│   ├── files/
│   └── tools/
│       ├── ToolName.py
│       ├── ToolName2.py
│       ├── ToolName3.py
│       ├── ...
├── example_agent2/
│   ├── __init__.py
│   ├── example_agent2.py
│   ├── instructions.md
│   ├── files/
│   └── tools/
│       ├── ToolName.py
│       ├── ToolName2.py
│       ├── ToolName3.py
│       ├── ...
├── agency.py
├── shared_instructions.md
├── requirements.txt
├── .env
└──...
```

**Rules for the folder structure:**

- Agency folder must be named in lowercase, with underscores instead of spaces.
- Each agency and agent has its own dedicated folder.
- Within each agent folder:

  - A 'tools' folder contains all tools for that agent.
  - An 'instructions.md' file provides agent-specific instructions.
  - An '**init**.py' file contains the import of the agent.

- Tool Import Process:

  - Create a file in the 'tools' folder with the same name as the tool class.
  - Tools are automatically imported to the agent class.
  - All new requirements must be added to the requirements.txt file.

- Agency Configuration:
  - The 'agency.py' file is the main file where all new agents are imported.
  - When creating a new agency folder, use descriptive names, like for example: marketing_agency, development_agency, etc.
  - Create a `.env` file in the root folder and add a placeholder for `OPENAI_API_KEY` and any other API keys that are required by the tools for the user to fill in.

Follow this folder structure when further creating or modifying any files. Replace example_agent folders with the actual agents when creating agents for the first time.

**To create a new agent template, use the following command:**

```bash
agency-swarm create-agent-template --description "Description of the agent" --model "gpt-5" --reasoning "medium" "agent_name"
```

### Best Practices for Agent Structures

- **Realistic Agent Roles**: Model agents after actual job positions, not task-specific roles
  - ✅ Good: "Data Analyst", "Campaign Manager", "Financial Advisor"
  - ❌ Bad: "Chart Creator", "Email Sender", "Report Generator"
- **Minimize Agent Count**: Start with 1 agent. Only add more if user explicitly requests or absolutely necessary
- **Role Consolidation**: If agents always work together, they should probably be one agent

# Step 3: Tool Creation

Tools are the specific actions that agents can perform. Tools can be either:

- MCP servers
- Custom tools that wrap API calls
- Built-in tools
- Custom tool that performs a specific action on local files (e.g., `WriteFile`)

Key characteristics of tools:

- **Standalone:** Tools must run independently with minimal dependencies on other tools or agents. Any agent should be able to use any tool without requiring additional prompting or coordination.
- **Configurable:** Tools should expose adjustable parameters (such as modes, thresholds, timeouts, limits, etc.) so agents can tune them to suit different environments or task requirements.
- **Composable:** The output format of each tool should match the input format of others wherever possible. This enables agents to autonomously chain tools together into workflows rather than relying on rigid, pre-defined sequences.

### Custom Tool Example

```python
# MyCustomTool.py
from agency_swarm.tools import BaseTool
from pydantic import Field
import os
from dotenv import load_dotenv

load_dotenv() # always load the environment variables

class MyCustomTool(BaseTool):
    """
    A brief description of what the custom tool does.
    The docstring should clearly explain the tool's purpose and functionality.
    It will be used by the agent to determine when to use this tool.
    """
    # Define the fields with descriptions using Pydantic Field
    example_field: str = Field(
        ..., description="Description of the example field, explaining its purpose and usage for the Agent."
    )

    def run(self):
        """
        The implementation of the run method, where the tool's main functionality is executed.
        This method should utilize the fields defined above to perform the task.
        """
        # Your custom tool logic goes here
        # Example:
        # account_id = "MY_ACCOUNT_ID"
        # api_key = os.getenv("MY_API_KEY") # or access_token = os.getenv("MY_ACCESS_TOKEN")
        # do_something(self.example_field, api_key, account_id)

        # Return the result of the tool's operation as a string
        return "Result of MyCustomTool operation"

if __name__ == "__main__":
    tool = MyCustomTool(example_field="example value")
    print(tool.run())
```

Remember, each tool code snippet you create must be IMMIDIATELY ready to use by the user. It must not contain any mocks, placeholders or hypothetical examples.

#### Agency Context (Shared State)

Agency context lets your tools and agents share data without passing it in conversation messages.

```python
from agency_swarm.tools import BaseTool

class MyTool(BaseTool):
    value: str = Field(..., description="The value to store in the context")
    def run(self):
        self._context.set("my_key", self.value)
        data = self._context.get("my_key", "default")
        return data
```

Use agency context for:

- Large data structures that are expensive to pass between agents
- Maintaining state across multiple tool calls
- Sharing data among tools and agents

Best practices:

- Use descriptive keys to avoid conflicts
- Provide default values when calling `get`
- Clean up unneeded data to keep the context small

### MCP Tools

Alternatively to creating custom tools, you can use special MCP servers which already contain predefined tools. In this case, you don't need to create custom tool files for the same functionality or add them to the PRD. You can use MCPs interchangeably with custom tools

```python
from agents.mcp import MCPServerStdio

filesystem_server = MCPServerStdio(
    # This name determines how the agent accesses the tools (e.g., Filesystem_Server.list_files)
    name="Filesystem_Server",
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", "."],
    },
    cache_tools_list=True
)
# Attach this server to your Agent via the mcp_servers list:
# my_agent = Agent(..., mcp_servers=[sse_server])
# Reference: https://agency-swarm.ai/core-framework/tools/mcp-integration#step-2-define-sse-server-connection-optional
```

For remote MCP servers, you can use the `MCPServerSse` or `MCPServerStreamableHttp` classes. See documentation for more details.

**Important**: Prefer creating MCP servers to connect to common platforms like Notion or Hubspot over custom tools. To find the best suited MCP servers, use web search.

### Built-in Tools

Agency Swarm has a built-in tools for web search and image generation. If the agent requires web searchs or image generation, you can simply include it in the agent's tools list.

```python
from agency_swarm.tools import WebSearchTool, ImageGenerationTool

tools = [WebSearchTool(), ImageGenerationTool()]
```

Unlike all other tools, WebSearch and ImageGeneration tools need to be initialized.

```python
from agency_swarm.tools import IPythonInterpreter, PersistentShellTool

tools = [IPythonInterpreter, PersistentShellTool]
```

### Best Practices

- **Real-World Actions Only**: Each tool must perform a specific, realistic action that a human would perform. Do not create abstract or speculative tools.
  - ✅ Good: "FetchInstagramLeads", "QueryDatabase", "SendSlackNotification"
  - ❌ Bad: "OptimizeText", "AnalyzeData", "MakeDecision"
- **Comments**: Well-commented code with clear step-by-step explanations ("# Step 1: ...", "# Step 2: ...")
- **Code Reliability**: Write production-ready functional code without placeholders or hypothetical examples
- **NEVER include API keys as tool inputs**: Always retrieve from environment variables using `os.getenv()` inside the `run` method
- **Use global variables for constants**: Define constant values (account_id, etc.) as global variables above the class, not in Pydantic `Field`
- **Add test case**: Include a test case in the `if __name__ == "__main__":` bloc for custom tools.
- **Avoid Redundant Logic**: Don't embed complex analysis logic inside tools. Let the agent do the analysis with simple tools
- **Keep Tools Single-Purpose**: Create atomic tools that perform specific tasks. The agent combines them for complex workflows

# Step 4: Instructions Writing

Each agent also needs to have an `instructions.md` file, which is the system prompt for the agent. Inside those instructions, you need to define the following:

- **Agent Role**: A description of the role of the agent.
- **Goals**: A list of goals that the agent should achieve, aligned with the agency's mission.
- **Process Workflow**: A step by step guide on how the agent should perform its tasks. Each step must be aligned with the other agents in the agency, and with the tools available to this agent.

Use the following template for the instructions.md file:

```md
# Role

You are **[insert role, e.g., "a helpful expert" or "a creative storyteller".]**

# Goals

- **[Insert high level goals for the business..(Eg. if you are building a report generator agent - increase sales by 10%)]**

# Process

## [Task Name]

**[Provide a step-by-step instructions process on how this task should be performed. Use a numbered list.]**

[...repeat for each task]

# Output Format

- **[Best suited output format for the agent (eg. "respond concisely and use simle language") or examples.]**

# Additional Notes

- **[Specify any additional notes here, if any. Use bullet points if needed.]**
```

### Best Practices

- **Start Simple**: Use concise, verb-driven instructions.
- **Be Specific**: Explicitly state desired outputs and formats.
- **Provide Examples**: Include concrete examples of expected behavior and tool usage.
- **Use Positive Instructions**: Phrase steps as "Do this" rather than "Don't do that".
- **Integrate Tools in Steps**: Show exactly when and how to use each tool in the workflow.
- **Output Formats**: Specify exact output schemas or formats for agent responses.
- **Iterate Continuously**: Refine instructions based on actual test results and feedback.
- **Avoid Speculation**: Be conscientious when creating instructions—avoid guessing or making unsupported assumptions. If certain information is not available, simply leave it blank so the user can fill it in.

# Step 5: Agency Creation

Agencies are collections of agents that work together to achieve a common goal. They are defined in the `agency.py` file, which already exists in the root folder.

1. **Import the agents into the `agency.py` file.**

   ```python
   from dotenv import load_dotenv
   from agency_swarm import Agency
   from ceo import ceo
   from developer import developer
   from virtual_assistant import virtual_assistant

   load_dotenv()

   # do not remove this method, it is used in the main.py file to deploy the agency (it has to be a method)
   def create_agency(load_threads_callback=None):
       agency = Agency(
           ceo,
           communication_flows=[
               (ceo, developer),
               (ceo, virtual_assistant),
               (developer, virtual_assistant),
           ],
           shared_instructions="shared_instructions.md",
       )
       return agency

   if __name__ == "__main__":
       agency = create_agency()
       agency.terminal_demo()

       # to test the agency, send a single prompt for testing:
       # async def main():
       #     response = await agency.get_response("Hello, how are you?")
       #     print(response)
       # asyncio.run(main())
   ```

   Agency must export a create_agency method, which is used for deployment.

   The first argument is the entry point for user communication. The communication flows are defined in the `communication_flows` parameter.

   **A Note on Communication Flows**:

   Communication flows are directional. In the `communication_flows` parameter above, the agent on the left can initiate conversations with the agent on the right.

2. **Define the `shared_instructions.md` file.**

   Shared instructions is a file that contains shared instructions for all agents in the agency. Typically, this file only contains a background (# Background) section that contains with information about the business, ICP, target audience, environment, etc. If user has not provided any information, create a template with headings but leave the content blank for the user to fill in.

# Step 6: Testing

The final step is to test each tool and the agency itself, to ensure they are working as expected.

1. First, install the dependencies for the agency using the following command:

   ```bash
   pip install -r requirements.txt
   ```

2. Then, run each tool file in the tools folder that you created, to ensure they are working as expected.

   ```bash
   python agent_name/tools/tool_name.py
   ```

   If any of the tools return an error, you need to fix the code in the tool file.

3. Once all tools are working as expected, you can test the agency by running the following command:

   ```bash
   python agency.py
   ```

   If the terminal demo runs successfully, you have successfully created an agency that works as expected.

**Important**: Please do not stop on this step until all new tools and agents have been tested and are working as expected. Do not ask for confirmation or wait for the user to respond. Just keep iterating until the agency performs as expected.

# Step 7: Iteration

Repeat the above steps as instructed by the user, until the agency performs consistently to the user's satisfaction. First, adjust the tools, then adjust the agents and instructions, then test again. Make sure to repeat each step accordingly.

If the user is not starting their agency from scratch, you can start from one of the steps above accordingly.

# Final Notes

1. NEVER output code snippets or file contents in the chat. Always create or modify the actual files in the file system. If you're unsure about a file's location or content, check the current folder structure and file contents before proceeding. If you find yourself about to output code in the chat, STOP and reconsider your approach.
2. Never create files with sample snippets, hypothetical examples or placeholders. When creating custom tools, you must create production-ready functional code.
3. Ensure all tools are properly tested before submitting your work to the user.
4. Follow the specified file creation order rigorously.
5. Create a to-do list for yourself with all the steps you need to complete before starting.
6. Don't start coding until you confirm virtual environment is activated.
7. Fetch agency swarm documentation if you need clarification on the framework.

# References

- [Creating PRD](.cursor/commands/create-prd.md)
- [Adding MCP Servers to Agents](.cursor/commands/add-mcp.md)
- [Writing Instructions for Agents](.cursor/commands/write-instructions.md)
- [Agency Swarm Full Documentation](https://agency-swarm.ai/llms.txt)


